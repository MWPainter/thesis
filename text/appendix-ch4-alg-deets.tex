%\begin{savequote}[8cm]
%\textlatin{Cor animalium, fundamentum e\longs t vitÃ¦, princeps omnium, Microco\longs mi Sol, a quo omnis vegetatio dependet, vigor omnis \& robur emanat.}
%
%The heart of animals is the foundation of their life, the sovereign of everything within them, the sun of their microcosm, that upon which all growth depends, from which all power proceeds.
%  \qauthor{--- William Harvey \cite{harvey_exercitatio_1628}}
%\end{savequote}

\chapter{\label{app:ar-algs}\label{app:algo-deets-ch4}Additional Algorithm Details For Chapter 4}

\todo{Currently a pasting ground for currting average returns }

This appendix covers any additional details for algorithms covered in Chapter 4 where 




\subsection{AR-BTS} 
\label{appsec:arbts}

    \todo{Write this out in full}

    \todo{This is original writing for AR-BTS}

    This search policy can still be used with average returns. The following is a summary of the definitions for \textit{Boltzmann Tree Search with Average Returns} (AR-BTS), which uses the value estimates of $\Varbts$ and $\Qarbts$ at each node, and temperature schedule $\alphaarbts(x) > 0$:
    %
    \begin{align}
        \piarbts(a|s) &= (1-\lambdaarbts)\rhoarbts(a|s) + \frac{\lambdaarbts}{|\cl{A}|}, 
                    \label{eq:arbts_search_policy} \\ 
        \rhobts(a|s) &\propto \exp\left(\frac{1}{\alphaarbts(N(s))}\left(\Qarbts(s,a)\right)\right).
                    \label{eq:arbts_value_policy}
    \end{align}
    %
    where $\epsarbts \in (0,\infty)$ is an exploration parameter and $\alphaarbts(x)$ is the search temperature schedule. Given a trajectory $\tau=(s_0,a_0,r_0,...,s_{h-1},a_{h-1},r_{h-1},s_h)$ and the leaf node value estimate $\tilde{r} = \Vinit(s_h)$, the value estimates are updated for $t=h-1,...,0$:
    \begin{align}
        \Qarbts(s_t,a_t) &\leftarrow 
            \frac{1}{N(s_t,a_t)} \left( (N(s_t,a_t)-1) \Qarbts(s_t,a_t) 
                + \tilde{r} + \sum_{i=t}^{h-1} r_i \right) \label{eq:4:arbts_backup_q} \\
        \Varbts(s_t) &\leftarrow 
            \frac{1}{N(s_t)} \left( (N(s_t)-1) \Varbts(s_t) 
                + \tilde{r} + \sum_{i=t}^{h-1} r_i \right). \label{eq:4:arbsts_backup_v} 
    \end{align}

    Similarly to BTS, either the Q-value estimates can be used for a recommendation policy or the most visited child node can be used:
    \begin{align}
        \psiarbts(s) &= \argmax_{a\in\cl{A}}\Qbts(s,a), \\
        \mvarbts(s) &= \argmax_{a\in\cl{A}} N(s,a).
    \end{align}


\subsection{AR-DENTS}
\label{appsec:ardents}

    \todo{Write this out in ful}


\subsection{MENTS, RENTS and TENTS with Average Returns} 
\label{appsec:arments}

    \todo{Its DENTS, with alpha \= beta \= const, and different entropy functions.}

    \todo{Justify using DENTS with these settings to apprixmate AR-MENTS etc by pointing at the DNETS mimicing MENTS search policy empirically,} Appendix \ref{appsec:dents_mimic_ments}.

    \todo{Below is c and p of original writing on this in the neurips paper}

    MENTS uses \textit{soft values}, $\hat{Q}_{\textnormal{sft}}$, which are not obvious how to replace with average returns. So to produce the AR variants of MENTS, RENTS and TENTS we use AR-DENTS as a starting point. \todo{ppara c and p from neurips, clean?}

    \paragraph{AR-MENTS.} For AR-MENTS we use AR-DENTS, but set $\beta(m)=\alpha(m)=\alpha_{\text{fix}}$. This algorithm resembles MENTS, as the weighting used for entropy in soft values is the same as the Boltzmann policy search temperature. \todo{ppara c and p from neurips, clean?}

    \paragraph{AR-RENTS.} To arrive at AR-RENTS, we replace any use of $\hat{Q}_{\textnormal{sft}}(s,a)$ with $\bar{Q}(s,a)+\beta(m)\cl{H}_Q(s,a)$. So we use Equations (\ref{appeq:average_return}), (\ref{appeq:entropy_v}) and (\ref{appeq:entropy_q}) for backups, but replace the Shannon entropy function $\cl{H}$, with a relative entropy function $\cl{H}_{\textnormal{relative}}$ in Equation (\ref{appeq:entropy_v}). The relative entropy function $\cl{H}_{\textnormal{relative}}$ uses the \textit{Kullback-Leibler divergence} between the search policy and the search policy of the parent decision node. The search policy used is the same as in RENTS, with the aforementioned substitution for soft values. See \todo{cite and or ref}%\cite{rents_your_tents}
        for full details on computing relative entropy and the search policy used in RENTS. \todo{ppara c and p from neurips, clean?}

    \paragraph{AR-TENTS.} Similarly, for AR-TENTS, we replace any use of $\hat{Q}^m_{\textnormal{sft}}(s,a)$ with $\bar{Q}^m(s,a)+\beta(m)\cl{H}_Q(s,a)$, and use Equations (\ref{appeq:average_return}), (\ref{appeq:entropy_v}) and (\ref{appeq:entropy_q}) for backups. This time, we replace the Shannon entropy function $\cl{H}$, with a Tsallis entropy function $\cl{H}_{\textnormal{Tsallis}}$ in Equation (\ref{appeq:entropy_v}). Again, we use the same search policy used in TENTS, with the substitution for soft values. See \todo{cite and or ref}%\cite{rents_your_tents}
        for how Tsallis entropy is computed and the corresponding search policy for TENTS. \todo{ppara c and p from neurips, clean?}

    \bd{although details of the tournaments is left to appendix <TODO>, of note is that each DP and AR set of algorithms were tuned using the same sequence of tournaments, and then each AR version was played off against the DP version to compare. In each case the AR versions won out and hence were used in the final round robin. <TODO clean up some words about why this might to better: my original hypothesis is that, like using the most visited recommendation poilcy, that the average returns has a smoothing effect on the recommendation poilcy, and lead the algorithms to being less sensitive to noise from the neural nets.>}

    \todo{Here was the writing where we talked about the above from original neurips}
    Additionally, we found that adapting the algorithms to use average returns (recall Equation (\ref{eq:uct_qbar})) outperformed using Bellman backups for Go (Appendix \ref{appsec:go_hps}). The Bellman backups were sensitive to and propogated noise from the neural network evaluations. We use the prefix `AR' to denote the algorithms using average returns, such as AR-DENTS. Full details for these algorithms are given in Appendix \ref{app:average_returns}.

