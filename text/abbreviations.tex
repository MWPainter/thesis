
% List of notation
\begin{mclistof}{List of Notation}{3.2cm}

    % Notes
    % i = free variable
    % j = free variable
    % k = free variable
    % t = free variable for current timestep

    % General maths
    \item[\Large\textbf{Global Notation}\hfill\hfill]
    \item[$\one$] 
        The indicator function, where $\one(A)=1$ when $A$ is true, and $\one(A)=0$ when $A$ is false. 
    \\
    %%%%%
    % 2.1 MDPs
    %%%%%
    \item[\Large\textbf{Markov Decision Processes (Section \ref{sec:2-1-mdps})}\hfill\hfill]
    \item[$\cl{A}$]
        A (finite) set of actions.
    \item[$H$]
        The finite-horizon time bound of an MDP.
    \item[$\cl{M}$]
        A Markov Decision Process, which is a tuple $\cl{M}=(\cl{S},s_0,\cl{A},R,p,H)$.
    \item[$p$] 
        The next-state transition distribution of an MDP.  $p(s' | s,a) : \cl{S} \times \cl{A} \times \cl{S} \rightarrow [0,1]$.
    \item[$\pi$]:
        A policy, mapping a state $s\in\cl{S}$ to a probability distribution over actions $\cl{A}$.
    \item[$R$] 
        The reward function of an MDP: $R(s,a) : \cl{S}\times \cl{A}\rightarrow \bb{R}$.
    \item[$\cl{S}$]
        A (finite) set of states.
    \item[$\suc{s}{a}$]
        The set of successor states of a state-action pair $(s,a)$, with respect to an MDP: $\suc{s}{a}=\{s'\in\cl{S}|p(s'|s,a)>0\}$.
    \item[$s_0$]
        $s_0\in\cl{S}$ is the initial starting state of an MDP.
    \item[$\tau$]
        A trajectory, or sequence, of states, actions and rewards that are sampled according to a policy $\pi$ and an MDP $\cl{M}$: $\tau = (s_0, a_0, r_0, s_1, ..., s_{H-1}, a_{H-1}, r_{H-1}, s_H)$.
    \item[$\tau_{i:j}$]
        A truncated trajectory, starting at timestep $i$, and ending at timestep $j$: $\tau_{i:j} = (s_i, a_i, r_i, s_{i+1}, ..., s_{j-1}, a_{j-1}, r_{j-1}, s_j)$. 
    \\
    %%%%%
    % 2.2 RL
    %%%%%
    \item[\Large\textbf{Reinforcement Learning (Section \ref{sec:2-2-rl})}\hfill\hfill]
    \item[$\alpha$] 
        The temperature parameter, or, the coefficient of the entropy term in the maximum entropy (soft) objective.
    \item[$\cl{H}$]
        The Shannon entropy, of a probability distribution or policy.
    \item[$J(\pi)$] 
        The objective function for (standard) reinforcement learning: $J(\pi) = V^{\pi}(s_0;0)$.
    \item[$J_{\sft}(\pi)$] 
        The objective function for maximum entropy (soft) reinforcement learning: $J_{\sft}(\pi) = V_{\sft}^{\pi}(s_0;0)$.
    \item[$\pi^*$]
        The optimal standard policy, that maximises the objective function $J(\pi)$
    \item[$\pi_{\sft}^*$]
        The optimal soft policy, that maximises the soft objective function $J_{\sft}(\pi)$
    \item[$Q^*$]
        The optimal Q-value function $Q^*(s,a;t)$ denotes the maximal expected value that can be achieved by any policy, starting from state $s_t=s$, with action $a_t=a$.
    \item[$\hat{Q}^k$]
        The estimate of the optimal value function after $k$ iterations of Value Iteration.
    \item[$Q^{\pi}$]
        The Q-value of a policy $\pi$. $Q^{\pi}(s,a;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, starting by taking action $a_t=a$.
    \item[$Q_{\sft}^*$]
        The optimal soft Q-value function $Q^*(s,a;t)$ denotes the maximal expected soft value that can be achieved by any policy, from state $s_t=s$ and taking action $a_t=a$. 
    \item[$\hat{Q}^k$]
        The estimate of the optimal soft value function after $k$ iterations of soft Value Iteration.
    \item[$Q_{\sft}^{\pi}$]
        The soft Q-value of a policy $\pi$. $Q_{\sft}^{\pi}(s,a;t)$ is the expected value of the policy from $s_t=s$, starting with action $a_t=a$, with an addition of the entropy of the policy weighted by the temperature $\alpha$.
    \item[$V^*$]
        The optimal value function $V^*(s;t)$ denotes the maximal expected value that can be achieved by any policy, starting from state $s_t=s$.
    \item[$\hat{V}^k$]
        The estimate of the optimal value function after $k$ iterations of Value Iteration.
    \item[$V^{\pi}$]
        The value of a policy $\pi$. $V^{\pi}(s;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$.
    \item[$V_{\sft}^*$]
        The optimal soft value function $V^*(s;t)$ denotes the maximal expected soft value that can be achieved by any policy, from state $s_t=s$. 
    \item[$V_{\sft}^{\pi}$]
        The soft value of a policy $\pi$. $V_{\sft}^{\pi}(s;t)$ is the expected value of the policy from $s_t=s$, with an addition of the entropy of the policy weighted by the temperature $\alpha$.
    \\
    %%%%%
    % 2.3 MABs
    %%%%%
    \item[\Large\textbf{Multi-Armed Bandits (Section \ref{sec:2-3-mab})}\hfill\hfill]
    \item[$x$] \todo{define here and move into correct place in the list}
    \item[$y$] \todo{define here and move into correct place in the list}
    \item[$f_i$] \todo{define here and move into correct place in the list}
    \item[$x^m$] \todo{define here and move into correct place in the list}
    \item[$y^m$] \todo{define here and move into correct place in the list}
    \item[$\bar{y}_i^m$] \todo{define here and move into correct place in the list}
    \item[$N$] \todo{define here and move into correct place in the list}
    \item[$\mu_i$] \todo{define here and move into correct place in the list}
    \item[$\mu^*$] \todo{define here and move into correct place in the list}
    \item[$\creg_{\mab}$] \todo{define here and move into correct place in the list}
    \item[$\pi(m)$] \todo{define here and move into correct place in the list. Maybe keep it as this and ask question}
    \item[$m$] \todo{define here and move into correct place in the list}
    \item[$\pi_{\ucb}(m)$] \todo{define here and move into correct place in the list}
    \item[$\pi^m$] \todo{define here and move into correct place in the list}
    \item[$\psi^m$] \todo{define here and move into correct place in the list}
    \item[$w$] \todo{define here and move into correct place in the list}
    \item[$w^m$] \todo{define here and move into correct place in the list}
    \item[$\mu_{w,i}$] \todo{define here and move into correct place in the list}
    \item[$\mu_{w}^*$] \todo{define here and move into correct place in the list}
    \item[$W$] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % 2.4 THTS
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Trial Based Heuristic Tree Search and Monte Carlo Tree Search (Section \ref{sec:2-4-thts})}\hfill\hfill}}]
    \item[$\backupv$]
        Updates the values at a decision node in the backup phase of a trial \thtspp, using the decision node's children, the trajectory sampled for the trial and the heuristic value function.
    \item[$\backupq$] 
        Updates the values at a chance node in the backup phase of a trial \thtspp, using the chance node's children, the trajectory sampled for the trial and the heuristic value function.
    \item[\mctsmode] 
        Specifies if \thtspp\ewe will sample a trajectory such that only one decision node is added to the search tree per trial. If not running in \mctsmode\ewe the \thtspp\ewe will sample a trajectory to the MDPs time horizon $H$.
    \item[$N(s)$]
        The number of visits at the decision node corresponding to state $s$.
    \item[$N(s,a)$]
        The number of visits at the chance node corresponding to state-action pair $(s,a)$.
    \item[$\node(s)$] 
        The decision node corresponding to the state $s$.
    \item[$\node(s)\dotchildren$]
        The set of chance nodes that are children of $\node(s)$.
    \item[$\dnodedata{s}$]
        The set of variables stored at decision node $\node(s)$, typically used for estimating values.
    \item[$\node(s,a)$] 
        The chance node corresponding to the state-action pair $(s,a)$.
    \item[$\node(s)\dotchildren$]
        The set of decision nodes that are children of $\node(s)$.
    \item[$\cnodedata{s,a}$] 
        The set of variables stored at decision node $\node(s,a)$, typically used for estimating Q-values.
    \item[$\pisearch$]
        The search policy used in \thtspp\ewe to sample a trajectory in the selection phase.
    \item[$\Qinit$]
        The heuristic action function used in \thtspp, used to provide a Q-value estimate for any state-action pairs that aren't in the search tree.
    \item[$\samplecontext$]
        A function used in \thtspp\ewe that creates a context, or key-value story, and samples any initial values to be stored in the context.
    \item[$\cl{T}$] 
        The \thtspp\ewe search tree. $\cl{T}\subseteq \cl{S} \cup \cl{S}\times\cl{A}$. 
    \item[$\cl{T}^k$] 
        The \thtspp\ewe search tree after $k$ trials have been run.
    \item[$\Vinit$]
        The heuristic value function used in \thtspp, used to initialise the value of a new decision node.

    \item[] \todo{Below is stuff from Section 2.4.X, which need to write up and sort into the list}
    
    \item[$n$] 
        Number of trials run.
    \item[$T$] 
        Computation time limit.

    \item[$\Vuct$] \todo{define here and move into correct place in the list}
    \item[$\Quct$] \todo{define here and move into correct place in the list}
    \item[$\piuct$] \todo{define here and move into correct place in the list}
    \item[$b_{\uct}$] \todo{define here and move into correct place in the list}

    \item[$\Vmcts$] \todo{define here and move into correct place in the list}
    \item[$\Qmcts$] \todo{define here and move into correct place in the list}
    \item[$\pimcts$] \todo{define here and move into correct place in the list}
    \item[$b_{\mcts}$] \todo{define here and move into correct place in the list}
    \item[$\tilde{V}_\theta$] \todo{define here and move into correct place in the list}
    \item[$\hat{V}^{\pirollout}$] \todo{define here and move into correct place in the list}
    \item[$\pirollout$] \todo{define here and move into correct place in the list}

    \item[$\Vments$] \todo{define here and move into correct place in the list}
    \item[$\Qments$] \todo{define here and move into correct place in the list}
    \item[$\piments$] \todo{define here and move into correct place in the list}
    \item[$\alpha_{\ments}$] \todo{define here and move into correct place in the list}
    \item[$\lambda_s$] \todo{define here and move into correct place in the list}
    \item[$\epsilon$] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % 2.5 MORL
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Multi-Objective Reinforcement Learning \\(Section \ref{sec:2-5-morl})}\hfill\hfill}}]
    \item[$\bfcl{M}$] \todo{define here and move into correct place in the list}
    \item[$\bff{\tau}$] \todo{define here and move into correct place in the list}
    \item[$\bff{\tau}_{:h}$] \todo{define here and move into correct place in the list}
    \item[$\bff{V}^{\pi}$] \todo{define here and move into correct place in the list}
    \item[$\bff{Q}^{\pi}$] \todo{define here and move into correct place in the list}
    \item[$\Delta^D$] \todo{define here and move into correct place in the list}
    \item[$u$] \todo{define here and move into correct place in the list}
    \item[$\bff{w}$] \todo{define here and move into correct place in the list}
    \item[$D$] \todo{define here and move into correct place in the list}
    \item[$\Pi$] \todo{define here and move into correct place in the list}
    \item[$U(\Pi;u)$] \todo{define here and move into correct place in the list}
    \item[$u_{\lin}$] \todo{define here and move into correct place in the list}
    \item[$CH(\Pi)$] \todo{define here and move into correct place in the list}
    \item[$CS(\Pi;u)$] \todo{define here and move into correct place in the list}
    \item[$CCS(\Pi)$] \todo{define here and move into correct place in the list}
    \item[$\valset(\Pi)$] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % 2.6 Sampling
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Sampling From Catagorical Distributions \ref{sec:2-6-sampling})}\hfill\hfill}}]
    \item[] \todo{define here and move into correct place in the list}
    \item[] \todo{define here and move into correct place in the list}
    \item[] \todo{define here and move into correct place in the list}
    \item[] \todo{define here and move into correct place in the list}

\end{mclistof} 

% List of abbreviations
\begin{mclistof}{List of Abbreviations}{3.2cm}

    
    %%%%%
    % abbrv - 2.1 MDPs
    %%%%%
    \item[\Large\textbf{Markov Decision Processes (Section \ref{sec:2-1-mdps})}\hfill\hfill]
    \item[MDP] Markov Decision Process
    %%%%%
    % abbrv - 2.2 RL
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Reinforcement Learning (Section \ref{sec:2-2-rl})}\hfill\hfill}}]
    \item[MDP] Markov Decision Process.
    \item[MENTS] Maximum ENtropy Tree Search.
    \\
    %%%%%
    % abbrv - 2.3 MABS
    %%%%%
    \item[\Large\textbf{Multi-Armed Bandits (Section \ref{sec:2-3-mab})}\hfill\hfill]
    \item[MAB] \todo{define here and move into correct place in the list}
    \item[UCB] \todo{define here and move into correct place in the list}
    \item[EMAB] \todo{define here and move into correct place in the list}
    \item[CMAB] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % abbrv - 2.4 THTS
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Trial Based Heuristic Tree Search and Monte Carlo Tree Search (Section \ref{sec:2-4-thts})}\hfill\hfill}}] 
    \item[MCTS] 
        Monte Carlo Tree Search.
    \item[MENTS] 
        Maximum ENtropy Tree Search.
    \item[THTS] 
        Trial-based Heuristic Tree Search.
    \item[\thtspp] 
        An extension of THTS used in this thesis.
    \item[UCT] 
        Upper Confidence Bound applied to Trees.
    \item[UCT-MCTS] 
        An MCTS variant of Upper Confidence Bound applied to Trees.
    \\
    %%%%%
    % abbrv - 2.5 MORL
    %%%%%
    \item[{\parbox[t]{\textwidth}{\Large\textbf{Multi-Objective Reinforcement Learning \\(Section \ref{sec:2-5-morl})}\hfill\hfill}}]
    \item[CHVI]
        Convex Hull Value Iteration.
    \item[CHVS]
        Convex Hull Value Set.
    \item[MOMDP]
        Multi-Objective Markov Decision Process.

    

\end{mclistof} 
