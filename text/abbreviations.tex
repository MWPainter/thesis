
% List of notation
\begin{mclistof}{List of Notation}{3.2cm}

    % Notes
    % i = free variable
    % j = free variable
    % k = free variable
    % t = free variable for current timestep

    % General maths
    \item[$\one$] 
        The indicator function, where $\one(A)=1$ when $A$ is true, and $\one(A)=0$ when $A$ is false. 
    \\
    %%%%%
    % 2.2 RL
    %%%%%
    \item[\Large\textbf{Multi-Armed Bandits (Section \ref{sec:2-1-mab})}\hfill\hfill]
    \item[$x$] \todo{define here and move into correct place in the list}
    \item[$y$] \todo{define here and move into correct place in the list}
    \item[$f_i$] \todo{define here and move into correct place in the list}
    \item[$x^m$] \todo{define here and move into correct place in the list}
    \item[$y^m$] \todo{define here and move into correct place in the list}
    \item[$\bar{y}_i^m$] \todo{define here and move into correct place in the list}
    \item[$N$] \todo{define here and move into correct place in the list}
    \item[$\mu_i$] \todo{define here and move into correct place in the list}
    \item[$\mu^*$] \todo{define here and move into correct place in the list}
    \item[$\creg_{\mab}$] \todo{define here and move into correct place in the list}
    \item[$\pi(m)$] \todo{define here and move into correct place in the list. Maybe keep it as this and ask question}
    \item[$m$] \todo{define here and move into correct place in the list}
    \item[$\pi_{\ucb}(m)$] \todo{define here and move into correct place in the list}
    \item[$\pi^m$] \todo{define here and move into correct place in the list}
    \item[$\psi^m$] \todo{define here and move into correct place in the list}
    \item[$w$] \todo{define here and move into correct place in the list}
    \item[$w^m$] \todo{define here and move into correct place in the list}
    \item[$\mu_{w,i}$] \todo{define here and move into correct place in the list}
    \item[$\mu_{w}^*$] \todo{define here and move into correct place in the list}
    \item[$W$] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % 2.2 RL
    %%%%%
    \item[\Large\textbf{Markov Decision Processes and Reinforcement Learning (Section \ref{sec:2-2-rl})}\hfill\hfill]
    \item[$\alpha$] 
        The temperature parameter. (The coefficient of the entropy term in the maximum entropy (soft) objective).
    \item[$\cl{A}$] 
        Set of actions in an MDP.
    \item[$H$] 
        The finite-horizon time bound of an MDP. 
    \item[$\cl{H}$]
        Shannon entropy, of a policy.
    \item[$J(\pi)$] 
        \todo{objective funciton}
    \item[$J_{\sft}(\pi)$] 
        \todo{soft objective funciton}
    \item[$p$] 
        The next-state probability distribution of an MDP.  $p(\cdot | s,a) : \cl{S} \times \cl{A} \times \cl{S} \rightarrow [0,1]$.
    \item[$\pi$]
        A policy, \todo{state that this is just for this chapter, see sect 2.2 defn for rest of thesis}
    \item[$\pi^*$]
        The optimal standard policy, that maximises the objective function $J(\pi)$
    \item[$\pi_{\sft}^*$]
        The optimal soft policy, that maximises the soft objective function $J_{\sft}(\pi)$
    \item[$Q^{\pi}$]
        The Q-value of a policy $\pi$. $Q^{\pi}(s,a;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, starting by taking action $a_{t+1}=a$.
    \item[$Q^*$]
        The optimal value function $Q^*(s,a;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$, starting with action $a_{t+1}=a$ by any policy.
    \item[$Q_{\sft}^{\pi}$]
        The Q-value of a policy $\pi$. $Q_{\sft}^{\pi}(s,a;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, starting by taking action $a_{t+1}=a$, \todo{plus the entopy of the policy weighted by the temperature alpha}. \todo{consider just writing max pi over the soft values here instead?}
    \item[$Q_{\sft}^*$]
        The optimal value function $Q_{\sft}^*(s,a;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$, starting with action $a_{t+1}=a$ by any policy, \todo{plus the entopy of the (optimal soft) policy weighted by the temperature alpha}.
    \item[$R$] 
        The reward function of in an MDP: $\cl{R}(s,a) : \cl{S}\times \cl{A}\rightarrow \bb{R}$.
    \item[$\cl{S}$] 
        Set of states in an MDP. 
    \item[$\tau$] 
        \todo{trajectory}
    \item[$\tau_{:h}$] 
        \todo{trajectory}
    \item[$t$] 
        \todo{free variable for current timestep? And generally add a list of free variables?}
    \item[$V^{\pi}$]
        The value of a policy $\pi$. $V^{\pi}(s;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$.
    \item[$V^*$]
        The optimal value function $V^*(s;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$ by any policy.
    \item[$V_{\sft}^{\pi}$]
        The soft value of a policy $\pi$. $V_{\sft}^{\pi}(s;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, \todo{plus the entopy of the policy weighted by the temperature alpha}.
    \item[$V_{\sft}^*$]
        The optimal soft value function $V^*(s;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$ by any policy, \todo{plus the entopy of the (optimal soft) policy weighted by the temperature alpha}.\todo{consider just writing max pi over the soft values here instead?}

    \item[$\cl{M}$] \todo{define here and move into correct place in the list}
    \\
    
    %%%%%
    % 2.3 THTS
    %%%%%
    \item[\Large\textbf{Trial Based Heuristic Tree Search (Section \ref{sec:2-2-thts})}\hfill\hfill]
    \item[$\cl{B}_Q$]
        \todo{define backup}
    \item[$\cl{B}_V$]
        \todo{define backup}
    \item[$N(s)$]
        The number of visits at the decision node corresponding to state $s$.
    \item[\mctsmode] 
        \todo{definition of} \mctsmode
    \item[$N(s)$]
        The number of visits at the decision node corresponding to state $s$.
    \item[$N(s,a)$]
        The number of visits at the chance node corresponding to state-action pair $(s,a)$.
    \item[$n$] 
        Number of trials run.
    \item[\node] 
        A mapping from states and state-action pairs to their corresponding decision and chance nodes respectively.
    \item[$\node(s).V$] 
        \todo{write this}
    \item[$\node(s,a).Q$] 
        \todo{write this}
    \item[$\pi$]
        A search policy, \todo{define, this is a parameter of thts++}. \todo{handle } $\pi^k$
    \item[$T$] 
        Computation time limit.
    \item[$\cl{T}$] 
        A THTS search tree. \todo{With:} $\cl{T}\subseteq \cl{S} \cup \cl{S}\times\cl{A}$. \todo{handle } $\cl{T}^k$
    \item[$\cl{T}^k$] 
        \todo{search tree on after k trials}
    \item[$\Vinit$]
        The initialisation function used in \thtspp, used to initialise the value of a new decision node.

    \item[$\Vuct$] \todo{define here and move into correct place in the list}
    \item[$\Quct$] \todo{define here and move into correct place in the list}
    \item[$\piuct$] \todo{define here and move into correct place in the list}
    \item[$b_{\uct}$] \todo{define here and move into correct place in the list}

    \item[$\Vmcts$] \todo{define here and move into correct place in the list}
    \item[$\Qmcts$] \todo{define here and move into correct place in the list}
    \item[$\pimcts$] \todo{define here and move into correct place in the list}
    \item[$b_{\mcts}$] \todo{define here and move into correct place in the list}

    \item[$\Vments$] \todo{define here and move into correct place in the list}
    \item[$\Qments$] \todo{define here and move into correct place in the list}
    \item[$\piments$] \todo{define here and move into correct place in the list}
    \item[$\alpha_{\ments}$] \todo{define here and move into correct place in the list}
    \item[$\lambda_s$] \todo{define here and move into correct place in the list}
    \item[$\epsilon$] \todo{define here and move into correct place in the list}
    \\

    %%%%%
    % 2.4 MORL
    %%%%%
    \item[\Large\textbf{Multi-Objective Reinforcement Learning (Section \ref{sec:2-4-morl})}\hfill\hfill]
    \item[$\bfcl{M}$] \todo{define here and move into correct place in the list}
    \item[$\bff{\tau}$] \todo{define here and move into correct place in the list}
    \item[$\bff{\tau}_{:h}$] \todo{define here and move into correct place in the list}
    \item[$\bff{V}^{\pi}$] \todo{define here and move into correct place in the list}
    \item[$\bff{Q}^{\pi}$] \todo{define here and move into correct place in the list}
    \item[$\Delta^d$] \todo{define here and move into correct place in the list}
    \item[$u$] \todo{define here and move into correct place in the list}
    \item[$\bff{w}$] \todo{define here and move into correct place in the list}
    \item[$d$] \todo{define here and move into correct place in the list}
    \item[$\Pi$] \todo{define here and move into correct place in the list}
    \item[$U(\Pi;u)$] \todo{define here and move into correct place in the list}
    \item[$u_{\lin}$] \todo{define here and move into correct place in the list}
    \item[$CH(\Pi)$] \todo{define here and move into correct place in the list}
    \item[$CS(\Pi;u)$] \todo{define here and move into correct place in the list}
    \item[$CCS(\Pi)$] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}
    \item[$ $] \todo{define here and move into correct place in the list}

\end{mclistof} 

% List of abbreviations
\begin{mclistof}{List of Abbreviations}{3.2cm}

    
    %%%%%
    % abbrv - 2.1 MABS
    %%%%%
    \item[\Large\textbf{Multi-Armed Bandits (Section \ref{sec:2-1-mab})}\hfill\hfill]
    \item[MAB] \todo{define here and move into correct place in the list}
    \item[UCB] \todo{define here and move into correct place in the list}
    \item[EMAB] \todo{define here and move into correct place in the list}
    \item[CMAB] \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % abbrv - 2.2 RL
    %%%%%
    \item[\Large\textbf{Markov Decision Processes and Reinforcement Learning (Section \ref{sec:2-2-rl})}\hfill\hfill]
    \item[MDP] Markov Decision Process.
    \\
    %%%%%
    % abbrv - 2.3 THTS
    %%%%%
    \item[\Large\textbf{Trial Based Heuristic Tree Search (Section \ref{sec:2-3-thts})}\hfill\hfill] 
    \item[MCTS] 
        Monte Carlo Tree Search.
    \item[MENTS]
        Maximum ENtropy Tree Search.
    \item[THTS] 
        Trial-based Heuristic Tree Search.
    \item[\thtspp] 
        \todo{thts++}
    \item[UCT] 
        Upper Confidence Bound applied to Trees.

    \item[\node] 
        \todo{define here and move into correct place in the list}
    \\
    %%%%%
    % abbrv - 2.4 MORL
    %%%%%
    \item[\Large\textbf{Multi-Objective Reinforcement Learning (Section \ref{sec:2-4-morl})}\hfill\hfill]
    \item[CHVI]
        Convex Hull Value Iteration
    \item[MOMDP]
        Multi-Objective Markov Decision Process

    

\end{mclistof} 
