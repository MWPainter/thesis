% List of notation
\begin{mclistof}{List of Notation}{3.2cm}

    % Notes
    % i = free variable
    % j = free variable
    % k = free variable
    % t = free variable for current timestep

    % General maths
    \item[$\one$] 
        The indicator function, where $\one(A)=1$ when $A$ is true, and $\one(A)=0$ when $A$ is false. 
    \\
    %%%%%
    % 2.1 RL
    %%%%%
    \item[\Large\textbf{Markov Decision Processes (Section \ref{sec:2-1-rl})}\hfill\hfill]
    \item[$\alpha$] 
        The temperature parameter. (The coefficient of the entropy term in the maximum entropy (soft) objective).
    \item[$\cl{A}$] 
        Set of actions in an MDP.
    \item[$H$] 
        The finite-horizon time bound of an MDP. 
    \item[$\cl{H}$]
        Shannon entropy, of a policy.
    \item[$J(\pi)$] 
        \todo{objective funciton}
    \item[$J_{\sft}(\pi)$] 
        \todo{soft objective funciton}
    \item[$p$] 
        The next-state probability distribution of an MDP.  $p(\cdot | s,a) : \cl{S} \times \cl{A} \times \cl{S} \rightarrow [0,1]$.
    \item[$\pi$]
        A policy, \todo{state that this is just for this chapter, see sect 2.2 defn for rest of thesis}
    \item[$\pi^*$]
        The optimal standard policy, that maximises the objective function $J(\pi)$
    \item[$\pi_{\sft}^*$]
        The optimal soft policy, that maximises the soft objective function $J_{\sft}(\pi)$
    \item[$Q^{\pi}$]
        The Q-value of a policy $\pi$. $Q^{\pi}(s,a;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, starting by taking action $a_{t+1}=a$.
    \item[$Q^*$]
        The optimal value function $Q^*(s,a;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$, starting with action $a_{t+1}=a$ by any policy.
    \item[$Q_{\sft}^{\pi}$]
        The Q-value of a policy $\pi$. $Q_{\sft}^{\pi}(s,a;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, starting by taking action $a_{t+1}=a$, \todo{plus the entopy of the policy weighted by the temperature alpha}. \todo{consider just writing max pi over the soft values here instead?}
    \item[$Q_{\sft}^*$]
        The optimal value function $Q_{\sft}^*(s,a;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$, starting with action $a_{t+1}=a$ by any policy, \todo{plus the entopy of the (optimal soft) policy weighted by the temperature alpha}.
    \item[$R$] 
        The reward function of in an MDP: $\cl{R}(s,a) : \cl{S}\times \cl{A}\rightarrow \bb{R}$.
    \item[$\cl{S}$] 
        Set of states in an MDP. 
    \item[$\tau$] 
        \todo{trajectory}
    \item[$\tau_{:h}$] 
        \todo{trajectory}
    \item[$t$] 
        \todo{free variable for current timestep? And generally add a list of free variables?}
    \item[$V^{\pi}$]
        The value of a policy $\pi$. $V^{\pi}(s;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$.
    \item[$V^*$]
        The optimal value function $V^*(s;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$ by any policy.
    \item[$V_{\sft}^{\pi}$]
        The soft value of a policy $\pi$. $V_{\sft}^{\pi}(s;t)$ denotes the expected cumulative reward that policy $\pi$ will achieve, starting from state $s_t=s$, \todo{plus the entopy of the policy weighted by the temperature alpha}.
    \item[$V_{\sft}^*$]
        The optimal soft value function $V^*(s;t)$ denotes the expected maximal value that can be achieved from state $s_t=s$ by any policy, \todo{plus the entopy of the (optimal soft) policy weighted by the temperature alpha}.\todo{consider just writing max pi over the soft values here instead?}
    \\
    
    %%%%%
    % 2.2 THTS
    %%%%%
    \item[\Large\textbf{Trial Based Heuristic Tree Search (Section \ref{sec:2-2-thts})}\hfill\hfill]
    \item[$\cl{B}_Q$]
        \todo{define backup}
    \item[$\cl{B}_V$]
        \todo{define backup}
    \item[$N(s)$]
        The number of visits at the decision node corresponding to state $s$.
    \item[\mctsmode] 
        \todo{definition of} \mctsmode
    \item[$N(s)$]
        The number of visits at the decision node corresponding to state $s$.
    \item[$N(s,a)$]
        The number of visits at the chance node corresponding to state-action pair $(s,a)$.
    \item[$n$] 
        Number of trials run.
    \item[\node] 
        A mapping from states and state-action pairs to their corresponding decision and chance nodes respectively.
    \item[$\node(s).V$] 
        \todo{write this}
    \item[$\node(s,a).Q$] 
        \todo{write this}
    \item[$\pi$]
        A search policy, \todo{define, this is a parameter of thts++}. \todo{handle } $\pi^k$
    \item[$T$] 
        Computation time limit.
    \item[$\cl{T}$] 
        A THTS search tree. \todo{With:} $\cl{T}\subseteq \cl{S} \cup \cl{S}\times\cl{A}$. \todo{handle } $\cl{T}^k$
    \item[$\cl{T}^k$] 
        \todo{search tree on after k trials}
    \item[$\Vinit$]
        The initialisation function used in \thtspp, used to initialise the value of a new decision node.

\end{mclistof} 

% List of abbreviations
\begin{mclistof}{List of Abbreviations}{3.2cm}

    
    %%%%%
    % abbrv - 2.1 RL
    %%%%%
    \item[\Large\textbf{Markov Decision Processes and Reinforcement Learning (Section \ref{sec:2-1-rl})}\hfill\hfill]
    \item[MDP] Markov Decision Process.
    \\
    % abbrv - 2.2 THTS
    \item[\Large\textbf{Trial Based Heuristic Tree Search (Section \ref{sec:2-2-thts})}\hfill\hfill] 
    \item[\CNODE] 
        The chance node class.
    \item[\cnode] 
        An instance of \CNODE, i.e. a chance node instance.
    \item[\DNODE] 
        The decision node class.
    \item[\dnode] 
        An intance of \DNODE, i.e. a decision node instance.
    \item[MCTS] 
        Monte Carlo Tree Search.
    \item[MENTS]
        Maximum ENtropy Tree Search.
    \item[THTS] 
        Trial-based Heuristic Tree Search.
    \item[\thtspp] 
        \todo{thts++}
    \item[UCT] 
        Upper Confidence Bound applied to Trees/
    

\end{mclistof} 
