% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}

\chapter{\label{ch:5-chmcts}Convex Hull Monte Carlo Tree Search} 

    \minitoc



    \bd{list, brain dumping it out.}
    
    This chapter\dots

    \begin{itemize}
        \item Sec 5.1, introduction. Starting with existing MOMCTS methods discussed in Ch3 \todo{ref}. These algorithms come in two flavours. One where a point estimate is maintained, and one where a convex hull is maintained. This section demonstrates why point estimates are not sufficient, and 
        \item Sec 5.2, convex hull monte carlo tree search
        \item Sec 5.3, contextual zooming for trees
        \item Sec 5.5, results
        \item Sec 5.6, theoretical results
    \end{itemize}

    \todo{link back to questions and how we're answering them here}


\section{Introduction}
\label{sec:5-1-intro}

    \todo{list}
    \begin{itemize}
        \item high level overview of CHMCTS work
        \item discuss how CHMCTS answers the research questions from introduction chapter
        \item moving into multi-objective land now
        \item Comment about CHVI and prior MOMCTS work motivating this
    \end{itemize} 
    
    First prior multi-objective MCTS (MOMCTS) algorithms are discussed. Specifically, 

    \bd{Point estimates insufficient can be copies from CHMCTS paper}

    \bd{In prior methods that use a convex hull} \todo{ref} \bd{, the environment is assumed to be deterministic, and backups can be seen to be a special case of Convex Hull backups from Chapter 2} \todo{ref}. \bd{Therefore, for the remainder of this chapter, most algorithms will use convex hull backups, similar to Convex Hull Value Iteration (todo ref), as in the deteminstic MOMDP case it reduces to the original algorithms, and extends them for stochastic MOMDPs. As such, this chapter, will predominantly focus on how perform action selection in MOMCTS (exploration?)}

    \bd{this paper (below) talks about the need for making consistent action selections. Because the aim is to discover for all weights w an optimal policy. Given a w, actions should be taken consistently to try and maximise the objective.}

    \bd{The hypervolume is the area under the convex hull. The optimal convex hull is the unique convex hull that maximised the hypervolume. The premise of some of the prior work is to maximise the hypervolume at every node. Or the premise is to select from the ``optimal actions'' that lie on the convex hull. However, reducing the problem back to discovering the optimal policies from the root node, this objective is a bit excessive. The optimal convex hull is not needed at non root nodes. } \todo{wrote a bit about this in confirmation in a wordy way, copy that?} \bd{Consider a 2d problem, with two actions, which correspond to optimising each of the objectives. Once a step has been taken in the MOMDP, the agent has in some sence committed to a weighting over the objectives, and it does not make sense for the agent to then select actions to optimise the other objective, as if the agent wanted to optimise the other objective it would have made a different first step. This has also been observed by XXX et al, who do YYY to solve this issue. In this work, a different solution is considered, by turning the problem into a contextual one.}

    \todo{want details in contextual tree search, but want to link to it from here. The detail above really should go below in sec 5.1.1}

    \bd{This example, and an example from XXX et al, are given in subsection TODO ref, to concretely demonstrate these issues.}

    \todo{Demonstrate the need for consistent action selection according to objectives. This paper talks about the same issue: https://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf, so should read, cite and discuss. They essentially use some policy tracking}
    





    
    \subsection{Contextual Tree Search}
    \label{sec:5-1-1-context} % used to be sec:5-2-context

        \todo{Talk about how values can be extracted from the convex hull objects given a context, and how this can be used to solve this consistent action problem. Refer back to section 2, and say that for decision support, when a policy is chosen, it implies a weight vector, which can be used to extract the policies.}

        \todo{use this to highlight that the intuition, of maximising the hypervolume or trying to find an optimal convex hull at every node is unnecessary. And leads to suboptimal behaviour and exploration.}

        \todo{Use the simple example of reward in dim 1 for action 1 and reward in dim 2 for action 2}

        \todo{Also make a simple 3 step example demonstrating what the vanmoffaert paper does, where there is a joint state on the two optimal paths. Which is an example of just picking actions from even the optimal convex hull at each step does not mean that you are following an optimal policy.}

        \bd{So the main things to highlight with these two examples. One, that taking the objective to compute each local convex hull (the full convex hulls at not root node) is not necessary, and leads to suboptimal exploration. Two, that even if you are given all of the optimal convex hulls at every state, that some form of context or history is required to follow a globally optimal policy.}

        \todo{Argue that in the case of decision support, if a context/weight vector can be obtained from the selection of a policy by the user, then the weight vector is a natural choice (and sufficient) for the context. As once given a context, the optimal choice is defined at every step. (And reference the theory section (and quote the result here).)}

        \todo{Front load results on the 2 action tree search. Demonstrate that the prior works dont do well on it, and show that ours does.}

        \todo{Introduce contextual (simple) regret}






        \todo{Below is brain dumping this section on 22 feb. Didn't look at CHMCTS paper at all when writing it.} 


        This subsection discusses the use of \textit{contextual tree search} for planning in multi-objective settings. \bd{And highlights why any multi-objective tree search algorithm needs to be contextual to run effectively through an example MOMDP.}
    
        \todo{Example MOMDP where the rewards are a circle. Have an image of the rewards, have an example of the 2 action 2D, and 3 action 2D envioronments. Also want to cite the EUM point generation that we used to generate points uniformly on a hypersphere, which allows the environment to be generate for an arbitrary (num actions, dimension) pair}
    
        In previous work \todo{refs back to litrev}, the rough intuition used is at every node in the tree that a full convex hull is desirable, and maps action selection in a variety of ways to try and achieve this.
    
        Using this example MOMDP \todo{reffering to the 3d one?}, where there are $M$ actions, it is quite clear from inspection what policies form the Convex Hull. $\phi^i(s)=a_{\cl{M}(i)}$ is the optimal policy for the \bd{weight vector} $\bff{w}^i=\bff{R}(\cdot,a_{\cl{M}(i)})$ \todo{probably want to fix up notation}. The convex hull is from inspection $\{\phi^i | 1\leq i \leq M\}$. 
    
        \bd{The point of this MOMDP is that really is that its easy to know optimal CH from inspection, and should be relatively easy to solve for any tree search algorithm.}
    
        To motivate why contextual tree search is a necessary component of MOMCTS, consider $s_0\rightarrow a_0 \rightarrow s_1$ \todo{clean notation} and that from state $s_1$ onwards, we know that the optimal action must be to continue taking $a_0$ until the trial ends. In some sense, the actions for the remainder of the trial should be consistent with any actions previously taken in the trial. Otherwise, the search policy will be essentially flipping between trying to optimse for different combinations of objectives and in effect optimising for none of them. \todo{This paragraph very brain dump}.
        
        The proposal in this thesis is for every trial to sample a \textit{context} weight vector, where for now it will be assumed that $\bff{w}^i \sim U(\Delta^D)$ \todo{just realised the Delta in BTS section might be overloaded notation? ALSO double check what notation used for simplex. ALso clean this equation up.} This context is then used to allow algorithms to follow a consistent objective over all actions in a trial. \todo{There may be other ways to achieve this, but the proposal of contextual tree search solves this problem relatively simply}
    
        \todo{Some experiments showing that the prior work on MOMCTS fails horribly on this MOMDP.}
    
    
    
        \bd{As the work in this chapter will follow the typical pattern of taking a multi-armed bandit problem and applying it to a sequential decision making problem, the theoretical quantity that will be considered in this chapter is contextual (cumulative) regret.}
    
        \todo{Define the typical MAB problem figure, and contextual regret equation.}
    
    
    
    
    
    
        \todo{HERE HERE HERE HERE. NExt = write up the typical MAB problem figure and defn contextual regret. With new command local. Then move onto defining contextual zooming for trees and CHMCTS. Doesn't matter if the notation is a bit off, as long as the equations which are slow to type out are typed out.}



        \todo{make defn}
        Given that the multi-objective problem is now being posed as a contextual problem, it is necessary to allow for policies to depend on the context weight. Given a weight $\bff{w}\in\Delta^D$, the probability of samping action $a$ from policy $\pi$ at state $s$ will now be written $\pi(a|s;\bff{w})$.

        Additionally, \textit{context dependent} policy will refer to policies that can vary for different weights, and \textit{context independent} policies are ones that give the same distribution for all weights. That is, a policy $\pi$ is context independent if $\pi(a|s;\bff{w}) = \pi(a|s;\bff{w}')$ for all $\bff{w},\bff{w}'\in\Delta^D$. \todo{How to handle that context dependent could learn a context independent? Maybe rephrase?}







    


    
\section{Contextual Zooming for Trees}
\label{sec:5-3-czt}

    \todo{part 1: say that a common approach from tree search is to think about each node as solving a (non-stationary) multi-armed bandit problem}

    \todo{part 2: following a similar approach, contextual zooming is an approach for contextual bandits }

    \todo{part 3: have a diagram of a box, UCB right to UCT, UCB down to CZ, UCT down to CZT, CZ right arrow to CZT}

    \todo{part 4: describe the algorithm. say that it's for a fixed number of trials. talk about doubling trick and how it can be used to bound contextual regret for unknown number of trials. In practise just use the equations without thinking about it.}

    \todo{part 5: subsection on CH-CZT, which is just using the convex hull backups for the recommendation policy.}







    A common design choice in tree search for sequential decision making is to use a relevant multi-armed bandit algoroithm at each deicison node of the tree. For example, the UCT algorithm adapts the UCB algorithm at each node, to run on a non-stationary MAB problem at each node. 

    Since Section \ref{sec:5-1-1-context} motivated viewing multi-objective MCTS from a contextual viewpoint. This Section considers utilising the \textit{Contextual Zooming} alogirithm \todo{ref in ch2} for contextual multi-armed bandits to give the \textit{Contextual Zooming for Trees} (CZT) algorithm.

    \bd{Following a similar methodology to UCT --- which runs UCB on a non-stationary multi-armed bandit problem at each node in the search tree --- \textit{Contextual Zooming for Trees} (CZT) runs \textit{Contextual Zooming} (Chapter 2, TODO REF) on a non-stationary contextual multi-armed bandit problem at each node in the search tree.}

    It's worth noting that because the CZT algorithm is following the ``UCT formula'', and that the contextual zooming algorithm is motivated in the \bd{standard} reinforcement learning setting, as opposed to the exploration setting. \bd{As such CZT in some sense is not designed with the exploration setting in mind.}

    \todo{Consdier this algo for two reasons: follow same design as UCT and contextual search policy baseline (as non  exist really).}

    % (This para was at the start, but dont want to start with it anymore)
    % \bd{taking a slight tangent from the exploration reinforcment learning setting, this section introduces Contextual Zooming for Trees, which will be used as a baseline contextual algorithm in experiments and considered for an action selection mechanism.}

    \todo{Box diagram}

    \todo{Revise contextual zooming algorithm}
        







    






\section{Convex Hull Monte Carlo Tree Search}
\label{sec:5-3-chmcts}

    \todo{part 1: give generalised CHMCTS backups}

    \todo{part 2: say that this generalises the prior works from ch3}

    \todo{part 3: state the search policies of the prior works, and label them}

    \todo{part 4: dont know if can be bothered to implement, but updating N(s) by w and making it a visitation vector now makes more sense than just a count.}






    This section introduces \textit{Convex Hull Monte Carlo Tree Search} (CHMCTS), which generalises the prior work in multi-objective MCTS \todo{ref ch3} to handle stochastic environments by using backups over \textit{Convex Hull Value Sets} from Chapter 2 \todo{ref ch2}. First the backups, which will be common across all of the following algorithms.

    Let ALG be a substitute acronym for any of the following algorithms. At each decision node $s$ a value set is maintained $\Valg(s)$, and at chance nodes $(s,a)$ the value set $\Qalg(s)$ is maintained. The search policy, $\pialg(\cdot|s,\bff{w})$, is \textit{contextual} and will depend on a randomly sampled $\bff{w}\in\Delta^D$ for each trial. \bd{Assume for now that the search policy is defined, as it will differ for each algorithm.}

    Given a context objective $\bff{w}\in\Delta^D$ and a trajectory $\tau=(s_0,a_0,\bff{r}_0,...,s_{h-1},a_{h-1},\bff{r}_{h-1},s_h)\sim\pialg$ \todo{want the w in the pialg bit somehow? Maybe make it superscript? But dont want it to mess with notation for theory.} the value estimates are updated for $t=h-1,...,0$:
    \begin{align}
        \Valg(s_t) &\leftarrow \cprune \left( \bigcup_{a_t\in\cl{A}} \Qalg(s_t,a_t) \right), 
            \label{eq:5:chmcts_v_backup} \\
        \Qalg(s_t,a_t) &\leftarrow \bff{R}(s_t,a_t) + \sum_{s'\in\suc{s_t}{a_t}} \frac{N(s')}{N(s_t,a_t)}\Valg(s'). 
            \label{eq:5:chmcts_q_backup}
    \end{align}

    where the $\cprune$ operation is defined in \eqref{eq:2:cprune_def}, \bd{but repeated here for clarity} over an arbitrary set of vectors $\bfcl{V}$:
    \begin{align}
        \cprune(\bfcl{V}) = \{\bff{v}\in\bfcl{V} | \exists \bff{w}\in\Delta^D. \forall \bff{v}'\in\bfcl{V}-\{\bff{v}\}. \bff{w}^\top \bff{v} > \bff{w}^\top \bff{v}' \}. \label{eq:5:cprune_def}
    \end{align}

    Additionally, because the search policies are contextual \todo{in the following subsections (todo ref)}, it will be useful to define \textit{contextual value estimates}, given a context objective $\bff{w}\in\Delta^D$ as follows:
    \begin{align}
        \Valgctx{\bff{w}}(s) &= \max \{\bff{w}^\top \bff{v} | \bff{v}\in\Valg(s) \}, \label{eq:5:contextual_value} \\
        \Qalgctx{\bff{w}}(s,a) &= \max \{\bff{w}^\top \bff{q} | \bff{q}\in\Qalg(s) \}. \label{eq:5:contextual_qvalue}
    \end{align} 

    \todo{define the policies for the algorithms in chapter 3. also think want to justify the equivalence, because they will use some } $\bar{\bff{R}}$ \todo{in the updates to the convex hulls. Decompose} $\bar{\bff{R}}(s_t) = \bff{R}(s_t,a_t) + \bar{\bff{R}}(s_{t+1})$, and say that this would only get added to CHVS at $s_t$ iff at $t+1$ the second part was added to the convex hull.

    \todo{should just write up the ch3 stuff before writing this tidbit}

    The algorithms described in Chapter 3 \todo{ref} can be realised using the above backups, with the search policies taking the following form:
    \begin{align}
        \pialg(s|\bff{w}) &= \argmax_{a\in\cl{A}} \left[
            \zetaalg(\bfcl{Q}(s,a)) + \calg \sqrt{\frac{\log(N(s))}{N(s,a)}} \right]
    \end{align}

    \todo{write up the policies for the ch3 algorithms by defining zeta for the different algorithms. And highlight the context independence from not depending on the weight}

    \todo{correct that pareto ucb uses a different search policy too}





    Below = previous writing on this

    \todo{list}
    \begin{itemize}
        \item Give convex hull monte carlo tree search
        \item Contextual zooming with the convex hull backups
    \end{itemize}



    \todo{Discuss property of extracting from convex hull}

    \todo{A range of action selection methods will be used.}

    Now that contextual tree search has been motivated, \textit{Convex Hull Monte Carlo Tree Search} is given in a form that generalises the works of \todo{cite lots}.

    \todo{CHMCTS backups}

    \todo{Contextual Search policy}

    \todo{Restate and name the prior works search policies}




    The algorithms considered in \todo{ref ch4} can be converted into contextual multi-objective algorithms by using the contextual values $\Valgctx{\bff{w}}(s)$ and $\Qalgctx{\bff{w}}(s,a)$ for decision making.







    \subsection{CH-CZT}
    \label{sec:5-3-1-chczt}

        \todo{move }


        Additionally, there is nothing preventing CZT from keeping convex hull value set estimates at each of it's nodes. This algorithm is referred to as Contextual Zooming for Trees with Convex Hull backups (CH-CZT). In such a case, the backups from \todo{ref eqns} are included, to update the value set estimates \todo{notation}, which are then used for recommnedations \todo{as follows}. In essence, this algorithm runs CZT, but adds an aditional convex hull backup which is used for recommendations.






        \todo{this should mostly be copying out from the chmcts paper.}




    \subsection{CH-UCT}
    \label{sec:5-3-2-chuct}

        \todo{part 1: give action selection for CH-UCT and talk about it}

        \todo{part 2: discuss more?}





        A natural way to extend the UCT search policy for contextual tree search is to use the contextual value estimates \eqref{eq:5:contextual_value} and \eqref{eq:5:contextual_qvalue} in the UCT search policy. Call this algorithm \textit{UCT with Convex Hulls} (CH-UCT). This gives the following search policy:
        \begin{align}
            \pichuct(s|\bff{w}) &= \argmax_{a\in\cl{A}} \left[
                \Qchuctctx{\bff{w}}(s,a) + \cchuct \sqrt{\frac{\log(N(s))}{N(s,a)}} \right].
        \end{align}

        \bd{One issue is that the confidence interval terms are shared between all possible context objectives, and may assign a high confident, or tight bound, for contexts that haven't been explored down this path much.} A hueristic way to solve this is to a vectorised visit counts $\bff{M}(s)$, $\bff{M}(s,a)$. These values are backed up after a trial for context $\bff{w}$ as follows:
        \begin{align}
            \bff{M}(s) \leftarrow \bff{M}(s) + \bff{w}, \\
            \bff{M}(s,a) \leftarrow \bff{M}(s,a) + \bff{w}.
        \end{align}

        Then a contextual visit count can be defined, and used in the search policy as follows:
        \begin{align}
            M(s;\bff{w}) &= \bff{w}^\top \bff{M}(s) \\
            M(s,a;\bff{w}) &= \bff{w}^\top \bff{M}(s,a) \\
            \pichuct(s|\bff{w}) &= \argmax_{a\in\cl{A}} \left[
                \Qchuctctx{\bff{w}}(s,a) + \cchuct \sqrt{\frac{\log(M(s;\bff{w}))}{M(s,a;\bff{w})}} \right].
        \end{align}

        Note that the original value of $N(s)$ is still needed in the convex hull backups for Q-value sets, to empirically estimate the transition distribution $p(s'|s,a)\approx \frac{N(s')}{N(s,a)}$, however this can be recovered from the vectorised visit count by summing over the values of the vector:
        \begin{align}
            N(s) &= \sum_{i=1}^D M(s)_i \\
            N(s,a) &= \sum_{i=1}^D M(s,a)_i.
        \end{align}

        \todo{notation above isn't clean.}
        




        \todo{Why doesnt a CH-UCT withe confidence bounds around the DP values you get from the convex hulls still work? Couldn't we do a k-d tree of visit counts for contexts, or some other way of counting how often a context has been searched for? There would be some lemma that the number of visits, is correct, for some epsilon ball around the weight, which the epsilon gives a small enough error in the value to be fine, except for the visit from the nodes above it in the kd tree. ANd then require exp many more trials to count for each level of the tree.}

        \todo{Add to ORIMenu to just submit some arxiv shit for something like ``an addendum to convex hull monte carlo tree search'', where we basically copy past this chapter. Actually, I think just take this chapter and publish it with the definitions cut down. Say for litreview either look at the CHMCTS or Thesis literature reviews.}

        \todo{Quote theory results}







    \subsection{CH-BTS}
    \label{sec:5-3-3-chbts}

        \todo{part 1: give action selection for CH-DENTS and talk about it}

        \todo{part 2: say CH-BTS is same without entropy backup}

        \todo{part 2: discuss entropy, say that it's just an approximate exploration bonus now}


        




    \subsection{CH-DENTS}
    \label{sec:5-3-4-chdents}

        \todo{part 1: give action selection for CH-DENTS and talk about it}

        \todo{part 2: say CH-BTS is same without entropy backup}

        \todo{part 2: discuss entropy, say that it's just an approximate exploration bonus now}














\section{Results}
\label{sec:5-4-results}

    \todo{part 0: discuss the evaluation prodceedure, and how the results are evaluated.}

    \todo{part 1: run algorithms on the tree envs that show the context dependent stuff better}

    \todo{part 1a: run algorithms with and without vectorised visit counts and show this helps with exploration?}

    \todo{part 2: run on suite of mo-gym environments}

    \todo{part 3: find environments where the convex hull blows up to motivate ch6. Give the CH fully and plot it out.}

    \todo{part 3a: also consider showing some of the optimal convex hulls where possible}




    \todo{list}
    \begin{itemize}
        \item Results from CHMCTS paper
        \item Get same plots from C++ code, but compare expected utility, rather than the confusing hypervolume ratio stuff
    \end{itemize}

    \bd{A range of algorithms will be considered in this experimental section. Firstly, for baselines, the convex hull backup versions of algorithms from Ch3 (REF) will be used. Additionally, CZT, CH-CZT, CH-UCT, CH-BTS and CH-DENTS will be considered}





    This section provides an empirical evalutation of the algorithms developed in this chapter. The baseline algorithms considered will also be CHMCTS algorithms, using the action selection methods outline in Sections \todo{ref litrev and above}. 

    \bd{This para is very braindump.} It is worth noting that in multi-objective MCTS algorithms that the accuracy and quality of the convex hull at the root node is the primary concern, even when a multi-objective MCTS algorithm is used in an \textit{online} manor, where tree search is interleaved between actions taken by the agent. Once a policy has been selected at the initial state it implicitly defines a weighting over the objectives as detailed in Section \todo{ref sec 2}. And following the discussion from \todo{ref above}, to follow an optimal policy the actions need to be taken consistently with respect to the same objective weights. As such, a more efficient online algorithm will be to run the multi-objective tree search from the initial state, and run a single-objective MCTS algorithm in a scalarised single-objective MDP for the remaining actions. Where in the scalarised reward is $R(s,a;\bff{w}) = \bff{w}^\top \bff{R}(s,a)$.
    



    \subsection{Evaluation Proceedure} 

        To evaluate the quality of the convex hull at the root node, the following metrics from the multi-objective literature \cite{morl_survey} are considered: \textit{Expected Utility Metric}, \textit{Hypervolume Metric} and a \textit{Sparsity Metric}. \todo{Add EUM and HV and Sparsity here or in ch2? }

        To evaluate the quality of the convex hull at the root node, where the optimal convex hull is not known a-priori, the \textit{Expected Utility Metric}, \textit{}

        Firstly, given a context $\bff{w}$ and CHMCTS search tree $\cl{T}$ that defines a partial recommendation poilicy $\psi_{\text{alg}}$, the policy will be made complete similarly to Section \todo{ref eval proceedure in ch4}:
        % 
        \begin{align}
            \texttt{complete}(\psi_{\text{alg}})(a|s;\bff{w}) =
            \begin{cases}
                1                       & \text{ if } s\in\cl{T} \text{ and } a=\psi_{\text{alg}}(s;\bff{w}), \\
                0                       & \text{ if } s\in\cl{T} \text{ and } a\neq\psi_{\text{alg}}(s;\bff{w}), \\
                \frac{1}{|\cl{A}|}      & \text{ otherwise.}
            \end{cases} \label{eq:full_Recommend}
        \end{align}
        % 
        \todo{should reword to account for CZT and how it does the same thing}

        The Convex Hull (of policies) can then be written as 
        \begin{align}
            \Psi_{\text{alg}} = \{\texttt{complete}(\psi_{\text{alg}})\ |\ \bff{w}\in\Delta^D \}.
        \end{align}

        This Convex Hull of policies is then evaluated using the \textit{Expected Utility Metric} (\eum), which is defined as:
        \begin{align}
            \eum(\Psi_{\text{alg}}) = \bb{E}_{\bff{w}\sim U(\Delta^D)}\left[
                \max_{\psi\in\Psi_{\text{alg}}} \bff{w}^\top \bff{V}^{\psi}(s_0)
             \right],
        \end{align}
        %
        where $U(\Delta^D)$ is the uniform distribution over vectors in $\Delta^D$. A monte-carlo estimate of $\eum(\Psi_{\text{alg}})$ will be computed, following \todo{cite the deep MORL paper}, let $E(\Delta^D)$ be an evenly spaced set of points in $\Delta^D$. Then the \eum\ewe can be approximated as:
        \begin{align}
            \eum(\Psi_{\text{alg}}) \approx \sum_{\bff{w}\in E(\Delta^D)} \bff{w}^\top \bff{V}^{\tau(\bff{w})},
        \end{align}
        %
        where $\tau(\bff{w}) \sim \texttt{complete}(\psi_{\text{alg}})$ is a sampled trajectory for the point $\bff{w}\in\Delta^D$, and $\bff{V}^{\tau(\bff{w})}$ is the cumulative return from the trajectory. That is, if \todo{double check indices are consistent with how been doing it before} the trajectory is $\tau(\bff{w})=(s_0,a_0,\bff{r}_0,...,s_{h-1},a_{h-1},\bff{r}_{h-1},s_h)$, then $\bff{V}^{\tau(\bff{w})} = \sum_{i=0}^{h-1} \bff{r}_i$.

        For the results given in \todo{ref}, the number of sampled used to approximate the \eum\ewe was XXX. That is $|E(\Delta^D)| = XXX$.

        \todo{double check this is consistent with the litrev paper on MORL and also cite it. Also what are the other metrics?}

        Experiments are run with \mctsmode\ewe turned off, as no memory issues were encountered during the evalutation.

        Similarly to the gridworld environments in Chapter \todo{ref}, the BayesOpt package \cite{bayesopt} was used to tune the hyperparameters of all algorithms in this chapter. Full details on the hyperparameter tuning and the selected parameters are given in Appendix \todo{new appendix? join the appendix?}.



    

    \subsection{Environments}

        The MO-Gymnasium \todo{cite} suite of environments was used to evaluate the algorithms. 

        \todo{Pick a couple envs to describe in more detail, and refer to cite for full details}

        Additionally, the Deep Sea Treasure is used, and the \todo{extended? (whatever the updated one is)} are used. \todo{Also mention that we can generate maps of varying size to perform a scalability analysis}


    



    
    \subsection{Results and Discussion}

        \todo{Paste in results and talk about them}
    


































\section{Theory}
\label{sec:5-5-theory}

    \todo{part 1: all the (relevant) search policies will select actions infinitely often.}

    \todo{part 1a: BTS and DENTS follows from ch4. For UCT can argue that if select one argument a finite number of times. Then use the confidence interval term to arrive at contradiction}

    \todo{part 2: convex hull backups perform the scalarised backups for all possible weights over the objective.}

    \todo{write up proofs wrote up on ipad. Used this gpt convo in cleaning up details: https://chatgpt.com/c/67fa9295-0cb8-8007-88fd-5bec1ac5393b}

    \todo{part 2a: refer back to convergence of backups in ch4, clean up and DKW stuff need to}

    \todo{part 3: if values converge to optimal, then contextual simple regret tends to zero}

    \todo{part 4: mention that CZT proofs would be much more complex, and given the other algorithms do better. Mention a lower bound which comes from the CZ regret bound. And that this algorithm could be preferable in not in exploring context.}

    \todo{part 4a: similar with CH-UCT to CZT}

    \todo{part 5: any proofs for CH-UCT and/or CZT? Which wouldn't be a disaster trying to proove?
    
    
    
    }




    Below is what we initially wrote in this section.

    \todo{See if with chatgpt help can crack out a rough proof outline? Because assuming no noise in the reward function, the leaf nodes clearly converge with same rate. Just really need the induction step. Feel like it should be very do-able, by bootstrapping on the CZT proofs.}

    \todo{Can easily show that CH converges, by using every action sampled infinitly often still, and bootstrap off the convex hull convergence proof.}

    \todo{One thing need to prove is that our form of backups converges, because we're using the sample averages our backups are slightly different. Should}

    \todo{Remainder of properties bootstrap off the proofs in Ch4. Every action is sampled inf often, hence the convex hull backups converge. Don't bother showing convergence rates. Just show that they are consistent.}

    \todo{Also want to show that given any convex hull value estimates, that following the policy extracted from convex hulls does give the same multi-objective value.}

    basically this would say
    \begin{align}
        \pi(s;w) &= \argmax_{a\in\cl{A}} \circ{\bff{Q}}(s,a;w) \\
        w^\top\bff{V}^\pi(s;w) &= w^\top \bb{E}_{a\sim\pi}[ \bff{Q}^\pi(s,a;w)] \\
            &= \circ{\bff{Q}}(s,a;w) \\
        w^\top\bff{Q}^\pi(s,a;w) &= ...    
    \end{align}

    So say that the value of the policy by taking the maimum from value estimates, will achieve the value estimates. We've been handwavy around empirical distriubtion vs actual transition distribution. But basically the argument is inductive, and holds trivially for leaf nodes.









