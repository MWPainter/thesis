% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}

\chapter{\label{ch:4-dents}Monte Carlo Tree Search With Boltzmann Exploration} 

    \minitoc

    This chapter discusses MCTS algorithms for planning in single-objective environments, while using entropy as an auxilary secondary objective for exploration. Notably, MENTS (Section \ref{sec:2-4-3-ments}), and RENTS and TENTS (Section \todo{ref litrev}) are already MCTS algorithms in the maximum entropy setting (Section \ref{sec:2-3-1-merl}), using entropy as an exploration bonus.

    In Section \ref{sec:4-1-intro} the motivation of the work 

\section{Introduction and Motivation}
\label{sec:4-1-intro}

    \todo{list}
    \begin{itemize}
        \item high level overview of DENTS work
        \item discuss how DENTS answers the research questions from introduction chapter
        \item state clearly that we're in single objective land here
        \item Comment about work exploring multi-armed bandits motivating this work
    \end{itemize}

    \todo{motivate stuff with the simulation stuff}

    This section highlights 

    \todo{Introduce DChain environments}

    \todo{Plots with all of the algorithms}

    \todo{Plot with MENTS temperature on x axis (need to run)}
    
    In the maximum entropy objective, it is argued that the standard objective can be recovered by setting $\alpha=0$ or setting $\alpha$ infitesimally small ($0<\alpha<<1$). \todo{add quote}. 
    %
    \todo{Although this is theoretically true (TODO ref the result about MENTS), in practise it is desirable to use the largest temperature that doesn't lead to undesirable (random) behaviour. In other words, extermely small temperatures do not utilise entropy for exploration effectively, while extremely large temperatures encourage agents to act randomly rather (reword: optimise for the standard objective). MENTS is used in (TODO) to demonstrate this issue with the maximum-entropy objective empirically, and (TODO) provides a corresponding theoretical result around MENTS.}
    %
    Although this is true, the most benefit can be gained from using entropy as an exploration bonus by setting a larger value of $\alpha$. This is highlighted in Figure \todo{ref}, where the performance on MENTS on the modified D-chain environment can be seen to improve as $\alpha$ is made larger, until a sudden drop off when it surpasses a threshold (\todo{at the poing 0.142ish}).
     
    Above the threshold, an agent can gain more ``reward'' by acting randomly and optimising for maximum-entropy, largely ignoring the rewards of the MDP. This example demonstrates an issue with using the maximum-entropy objective, where the temperature parameter needs to be finely tuned to a sweet-spot for it to be effective.

    This issue can be exasurbated further by editing the D-chain environment to contain an entropy trap. In the entropy trap environment \todo{ref fig}, an additional trap for maximum-entropy agents is added to the D-chain environment. In this environment, when the end of the chain is reached, there are two choices, to either collect the immediate reward of $1$, or to follow another path where high entropy can be obtained, but no reward can be obtained. The maximum entropy that can be obtained from following the chain is $\alpha K \log(2)$, and as such will mislead any maximum-entropy agents that have a temperature parameter of $\alpha > \frac{1}{k\log(2)}$. Setting \todo{k=what}, sets this threshold at \todo{todo say $\alpha > C$ for now}. However, for $\alpha \leq C$, the temperature parameter is low enough that not much benefit is gained from the entropy objective.

    Figure \todo{ref} shows the performance of ments on the entropy trap environment, highlighting that it doesn't perform well for any setting of $\alpha$. In contrast, the algorithms defined in this chapter are able to overcome these issues.
    
    As such, this chapter will consider how entropy can be used as an additional secondary objective, while still focusing on performing well in the standard objective.


    \todo{define simple regret}

    \todo{define consistency, use exploring bandits to say that consistent in regret implies consistent in simple regret?}

    \todo{Talk about the setup we're using. Probably want to try motive similarly to DENTS paper, and recall the diagram from} \ref{sec:2-3-rl}. \todo{Say that in fig below that normally step 3 is not considered}

    \begin{figure}
        \begin{tcolorbox}
            Parameters: An MDP $\cl{M}$.
            \begin{itemize}
                \item For each round $m=1,2,...$:
                \begin{enumerate}
                    \item the agent produces a search policy $\pi^m$ to follow;
                    \item the environment samples a trajectory $\tau\sim\pi^m$ (including rewards $r_t=R(s_t,a_t)$ for each $s_t,a_t$ pair in $\tau$);
                    \item the agent produces a recommendation policy $\psi^m$;
                    \item if the environment sends a stop signal, then the game ends, otherwise the next round starts.
                \end{enumerate} 
            \end{itemize}
        \end{tcolorbox}
        \caption{The procedure of an exploring planning problem for MDPs. \todo{Make this sound better.} \todo{Also make ch2 MAB figs use enumerate?}}
        \label{fig:3:planning_problem}
    \end{figure}

    \todo{make this a proper def}
    Simple regret of a policy $\psi$ is then defined as
    \begin{align}
        \sreg(s,\psi) = V^*(s)-V^{\psi}(s).
    \end{align}

    \todo{make this a proper def}
    An agent, that produces recommendation policies $\psi^1,\psi^2,...$ is said to be \textit{consistent} if $\bb{E}[\sreg(s,\psi^m)] \rightarrow 0$ as $m\rightarrow \infty$.

    \todo{Want some plots of MENTS temperature (x axis) vs performance, on gridworld and theory envs? Basically want something showing that optimal temp is in middle}








\section{Boltzmann Search}
\label{sec:4-2-boltzmannsearch}

    \todo{list}
    \begin{itemize}
        \item Recall MENTS
        \item Define BTS using THTS functions
        \item Define DENTS using THTS functions
        \item Discuss alias method variant (and complexity analysis) in a subsection?
    \end{itemize}

    \todo{would like to read more about the exp3 stuff before submitting this ch}
    \todo{https://tor-lattimore.com/downloads/book/book.pdf - bandits book}
    \todo{exp3 paper: http://rob.schapire.net/papers/AuerCeFrSc01.pdf}
    \todo{add to future work ideas to adapt BTS to use exp3 type stuff, and the thing gradient update covered in the Sutton and Barto book (todo get link and page ref to RL book where talk about that)}
    \todo{moved already}

    This section defines two algorithms Boltzmann Tree Search and Decaying ENtropy Tree Search in the \thtspp\ewe schema. 
    
    \subsection{Boltzmann Tree Search}

        \todo{add label}
        \todo{define BTS in thtspp}

        \todo{Define with variable alpha, change proofs to say for fixed alpha get regret bound. And add theorem that }


        Our first approach, put simply, replaces the use of soft values in MENTS with 
        % \textit{dynamic programming} (DP) 
        \textit{Bellman} 
        values. We call this algorithm \textit{Boltzmann Tree Search} (BTS).  The search policy $\pi_{\textnormal{BTS}}$ and backups for the $n$th trial are given by:
        %


        This section introduces the \textit{Boltzmann Tree Search} (BTS) algorithm, presented in terms of the \thtspp\ewe schema \todo{ref}. BTS promotes exploration through the stochastic Boltzmann search policy, like MENTS \todo{ref}, while using backups that optimise for the standard objective, like UCT \todo{ref}. BTS uses Bellman value estimates at each node $\Vbts$ and $\Qbts$. The search policy is defined by:
        %
        \begin{align}
            \pibts(a|s) &= (1-\lambda_s)\rhobts(a|s) + \frac{\lambda_s}{|\cl{A}|}, 
                        \label{eq:bts_search_policy} \\ 
            \rhobts(a|s) &\propto \exp\left(\frac{1}{\alpha}\left(\Qbts(s,a)\right)\right).
                        \label{eq:bts_value_policy}
        \end{align}

        And given a trajectory $\tau=(s_0,a_0,r_0,...,s_{h-1},a_{h-1},r_{h-1},s_h)$ the value estimates are updated for $t=h-1,...,0$:
        \begin{align}
            \Qbts(s_t,a_t) &\leftarrow 
                R(s_t,a_t) + \sum_{s' \in \suc{s_t}{a_t}} \left( \frac{N(s')}{N(s_t,a_t)} \Vbts(s) \right), 
                        \label{eq:bts_q_backup} \\ 
            \Vbts(s_t) &\leftarrow \max_{a\in\cl{A}} \Qbts(s_t,a), 
                        \label{eq:bts_v_backup} 
        \end{align}
        %
        where $\lambda_s=\min(1,\epsilon/\log(e+N(s)))$, $\epsilon \in (0,\infty)$ is an exploration parameter and $\alpha$ is a search temperature (unrelated to entropy). \todo{$\alpha \rightarrow \alpha(t)$}.

        \todo{Go through below to make work in thesis, just got it compiling because sleepy}

        \todo{talk about value initialisation}
        %Each $\hat{V}(s)$ and $\hat{Q}(s,a)$ are initialised using $V^{\text{init}}$ and $Q^{\text{init}}$ functions similarly to MENTS. 
        
        When BTS needs to recommend a policy, it can use it's Q-value estimates:
        %
        \begin{align}
            \psibts(s)=\argmax_{a\in\cl{A}}\Qbts(s,a).
        \end{align}

        \todo{add comments to make latex directly read a bit nicer, but also not having spacing between paragraphs etc. Also make equations nicer in ch2}

        %
        By using 
        % DP 
        Bellman
        backups, we can guarantee that the BTS recommendation policy converges to the optimal standard policy for any temperature $\alpha$, given enough time. In other words, BTS is consistent.
        %
        \begin{theorem} 
            \label{thrm:bts}
            For any MDP $\cl{M}$, after running $n$ trials of the BTS algorithm with a root node of $s_0$, there exists constants $C,k>0$ such that for all $\varepsilon>0$ we have $\bb{E}[\sreg(s_0,\psibts)] \leq C\exp(-kn)$, and also $\Vbts(s_0) \rap V^*(s_0)$ as $n\rightarrow\infty$.
        \end{theorem}

        \todo{update for $\alpha$ being const etc. Add the more theorems about BTS here.}
        % \begin{proofoutline}
        % 		This result is a special case of Theorem \ref{thrm:dents} by setting $\beta(m)=0$.
        % \end{proofoutline}
        % \begin{proof}
        %     Proofs for Theorem \ref{thrm:bts} and Theorem \ref{thrm:dents} provided in Appendix \ref{app:proofs}.
        % \end{proof}
    
    \subsection{Decaying ENtropy Tree Search}

        \todo{add label}
        \todo{define DENTS in thtspp}

        The \textit{entropy values} are weighted by a bounded non-negative function $\beta(N(s))$ in the DENTS search policy $\pidents$:
        %
        \begin{align}
            \pidents(a|s) &= (1-\lambda_s)\rhodents(a|s) + \frac{\lambda_s}{|\cl{A}|}, 
                        \label{eq:dents_search_policy} \\ 
            \rhodents(a|s) &\propto \exp\left(\frac{1}{\alpha}\left(\Qdents(s,a)+\beta(N(s))\HQdents(s,a)\right)\right).
                        \label{eq:dents_value_policy}
        \end{align}

        And given a trajectory $\tau=(s_0,a_0,r_0,...,s_{h-1},a_{h-1},r_{h-1},s_h)$ the value estimates are updated for $t=h-1,...,0$:
        \begin{align}
            \Qdents(s_t,a_t) &\leftarrow 
                R(s_t,a_t) + \sum_{s' \in \suc{s_t}{a_t}} \left( \frac{N(s')}{N(s_t,a_t)} \Vbts(s) \right), 
                        \label{eq:dents_q_backup} \\ 
            \Vdents(s_t) &\leftarrow \max_{a\in\cl{A}} \Qbts(s_t,a), 
                        \label{eq:dents_v_backup} 
        \end{align}
        % 
        and the \textit{entropy values} $\HVdents(s)$ and $\HQdents(s,a)$ (which are monte-carlo estimates of the entropy of the search policy rooted at $s$ and $(s,a)$ respectively) are updated given $\tau$ as follows:
        \begin{align}
            \HVdents(s_t) &\leftarrow \cl{H}(\pidents(\cdot | s_t)) + \sum_{a\in\cl{A}} \pidents(a|s_t)\HQdents(s_t,a), \label{eq:dents_entropy_v_backup} \\
            \HQdents(s_t,a_t) &\leftarrow \sum_{s'\in \suc{s_t}{a_t}} \frac{N(s')}{N(s_t,a_t)} \HVdents(s').  \label{eq:dents_entropy_q_backup}
        \end{align}



    \subsection{Advantages of Stochastic Search Policies}

        \todo{add label}
        \todo{write about alias sampling}

    % \subsection{Encorporating Prior Knowledge}

        \todo{add label}
        \todo{write about using $\tilde{\pi}$, $\tilde{Q}$, $\tilde{V}$.}
        \todo{talk about grid world and how Qinit need to be able to handle costs}

        MENTS suggests using $\tilde{\pi}$ as \todo{their description}

        However, this may still lead to issues in for example in grid world problems. \todo{talk about }





\section{Toy Environments}
\label{sec:4-3-toyenvs}

    \todo{list}
    \begin{itemize}
        \item Define D-chain stuff from the paper
        \item Define the D-chain with entropy trap
        \item Front load some results still
    \end{itemize}

\section{Theoretical Results}
\label{sec:4-4-theory}

    \todo{list}
    \begin{itemize}
        \item add theoretical results
    \end{itemize}

    Should we try to add a result about most visited? Where will be concentration bounds around the number of visits being proportional to the values. So think can bootstrap those proofs.

\section{Empirical Results}
\label{sec:4-5-results}

    \todo{list}
    \begin{itemize}
        \item DChain
        \item GridWorlds
        \item Go
    \end{itemize}

\section{Full Results}
\label{sec:4-6-fullresults}

    \todo{there's a lot of figures for the D-chain environment, work out how to best fit them in? Or put them in this seperate section?}