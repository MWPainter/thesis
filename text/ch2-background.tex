% \begin{savequote}[8cm]
% Alles Gescheite ist schon gedacht worden.\\
% Man muss nur versuchen, es noch einmal zu denken.

% All intelligent thoughts have already been thought;\\
% what is necessary is only to try to think them again.
%   \qauthor{--- Johann Wolfgang von Goethe \cite{von_goethe_wilhelm_1829}}
% \end{savequote}

\chapter{\label{ch:2-background}Background}

    \minitoc

    \todo{Introduce that going to introduce notation and give the building blocks this thesis builds off}

    \todo{Nicks comment to revisit: In Sections 2 and 3 you present RL before tree search. This was a surprise to me. Wouldn’t it be better to start with single objective MDPs, then introduce methods of solving them (planning, bandits, rl) and the different assumptions they make? Then move to MO versions. I’d also be open to you presenting MCTS then generalising to MCTS rather than the other way around, but I think the way you present it is probably the most efficient.}

    \todo{Add a regret or multi-armed bandit section?}

% \section{Multi-Armed Bandits}
% \label{sec:2-0-mab}

%     \todo{Introduce tree search using multi-armed bandits?}
%     \hide{
%     \begin{itemize}
%         \item Would like to think a bit about some of the bandits work that sample actions (from adversarial I think), because they were similar to boltzmann search but I hadn't seen details about those works when writing dents
%         \item Also the gradient based MAB stuff in sutton and barto book? Looks relevant? Maybe consider that as update to DENTS paper? Either way, another idea for getting good Go results.
%     \end{itemize}
%     }
%     \todo{list}
%     \begin{itemize}
%         \item $R(s,a)$ is a random variable in MAB literature, but we're assuming it's a fixed value in RL
%         \item Multi-Armed Bandits routines algos
%         \item Exploring Bandits routines and algos
%         \item Contextual Bandits routines and algos
%     \end{itemize}

\section{Markov Decision Processes and Reinforcement Learning}
\label{sec:2-1-rl}

    \todo{list}
    \begin{itemize}
        \item Typical agent interacting with environment diagram 
        \item Agent planning with simulator 
        \item \st{MDPs definition}
        \item \st{Policies}
        \item \st{Value functions (single (and multi-objective?))}
        \item \st{Basic results and definitions we use (tabular planning algorithms)}
        \item \st{Talk about entropy and some of that work (probably a subsection)}
    \end{itemize}

    \todo{Split this section into MDPs definition and move RL below monte carlo tree search? Probably if do the multi-armed bandit stuff.}

    In this section Markov Decision Processes are introduced, as well as some fundamental concepts in Reinforcement Learning such as \textit{policies} and \textit{value functions} which will be useful in the following. Afterwards, the \textit{maximum entropy objective} is discussed, as it will be relevant to some of the work in this thesis and other closely related work. 

    In this thesis we will only consider discrete and finite-horizon Markov Decision Processes, \todo{but we will point to works that make adaptions for non-finite horizons and continuous and partially observable environments, and briefly describe why the ideas could be used in parallel with the concepts in this thesis.}

    \begin{defn}
        \label{def:mdp}
        A \textnormal{Markov Decision Process} (MDP) is a tuple $\cl{M}=(\cl{S},s_0,\cl{A},\cl{R},p,H)$, where $\cl{S}$ is a set of states, $s_0\in\cl{S}$ is an initial state, $\cl{A}$ is a set of actions, $R(s,a)$ is a reward function $\cl{S}\times \cl{A}\rightarrow \bb{R}$, $p(\cdot | s,a)$ is a next state transition distribution $\cl{S} \times \cl{A} \times \cl{S} \rightarrow [0,1]$ and $H\in\bb{N}$ is a finite-horizon time bound. 
    \end{defn}

    Where it is more convenient, we will use the following notation to refer to the set of successor states when discussing MDPs:
    \begin{defn}
        \label{def:succ}
        The set of \textnormal{successor states} $\suc{s}{a}$ of a state-action pair $(s,a)$ is defined as $\suc{s}{a}=\{s'|p(s'|s,a)>0\}$. Additionally, let $s'\sim \suc{s}{a}$ be a shorthand for $s'\sim p(\cdot|s,a)$.
    \end{defn}

    \todo{decide if should keep p(cdot|s,a), and just swap out the succ command as shorthand for that?}

    Formally, we will consider a policy to be a mapping from a state in $\cl{S}$ to a distribution over actions $\cl{A}$. \todo{For this thesis we will consider policies to be stochastic, so when a policy is used to generate actions to follow, it is done by sampling from the distribution.}

    \todo{So im using pi for the definition here. But for rest of thesis it will be used as explicitly the "search policy"}

    In some cases it will be necessary to talk about deterministic policies. \todo{When ambiguous we will state when a policy is deterministic. A deterministic policy is written as a one hot stochastic distribution}

    \begin{defn}
        \label{def:policy}
        A \textnormal{(stochastic) policy} $\pi:\cl{S}\rightarrow \cl{A} \rightarrow [0,1]$ is a mapping from states to distributions over actions, where for all $s \in \cl{S}$ we have $\sum_{a\in\cl{A}} \pi(a|s) = 1$. We will use the following notations: $\pi(\cdot|s)$ is used to denote the entire distribution over the set of actions, $\pi(a|s)$ denotes the probability that action $a$ is sampled by the policy at state $s$. 

        Additionally, for \textnormal{deterministic policies} we will use the shorthand notation $\pi(s)$ to denote the action taken by the policy. Formally, we will write $\pi(s)=a'$ where $a'\in\cl{A}$ is such that $\pi(a'|s)=1$.

        \todo{want to add a statement saying something like} $\pi(s)=a'$ is shorthand for $\pi(a|s)=\one[a=a']$.

        \todo{Actually want to say something like:} a deterministic policy is special case of the stochastic, with a delta distribution, such as $\pi(a|s)=\one[a=a']$, and in such cases we will use $\pi(s)=a'$ as a short hand to state $\pi(a|s)=\one[a=a']$ and $\pi(s)$ as a shorthand for $a'$.
    \end{defn}

    \todo{Explicitly note that we are using pi as a free variable (is free variable correct word here?) here, and that in section - todo ref - we will use pi to refer to a specific type of policy (search policies) in the rest of the thesis after this section.}

    \todo{clean up policy definition with shorthand stuff}

    In reinforcement learning it is common to refer to a sequence of states, actions and rewards as a \textit{trajectory}. In the tree search literature it is more common to refer to this as a \textit{trial} \todo{cite thts?}, so we will use these terms interchangeably.
    
    \begin{defn}
        \label{def:trajectory}
        A \textnormal{trajectory}, is a sequence of state, action and rewards, that is induced by a policy $\pi$ and MDP $\cl{M}$ pair. Let the trials/trajectory be $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_{H-1}, a_{H-1}, r_{H-1}, s_H)$, where $a_t \sim \pi(\cdot|s_t)$, $r_t=R(s_t,a_t)$ and $s_{t+1} \sim \suc{s_t}{a_t}$. Notationally, we will write $\tau\sim\pi$ to denote a sampled trial/trajectory with respect to a policy, where the MDP is implicit.

        Sometimes it will be necessary to reason about trajectories with a horizon $h<H$, which will be denoted $\tau_{:h} = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_{h-1}, a_{h-1}, r_{h-1}, s_h)$.
    \end{defn}

    Next the value and Q-value of a policy is defined, which is the expected cumulative reward that a policy will obtain:
    \begin{defn}
        \label{def:value}
        \label{def:q_value}
        The \textnormal{value} of a policy $\pi$ from state $s$ at time $t$ is:
        \begin{align}
            V^{\pi}(s;t) = \bb{E}_{\tau\sim\pi}\left[\sum_{i=t}^{H-1} r_t \Bigg| s_t=s \right].
        \end{align} 

        The \textnormal{Q-value} of a policy $\pi$, from state $s$, with action $a$, at time $t$ is:
        \begin{align}
            Q^{\pi}(s,a;t) = R(s,a) + \bb{E}_{s'\sim \suc{s}{a}} [V^{\pi}(s';t+1)].
        \end{align} 
    \end{defn}

    From the definition of the values functions the optimal value functions can be defined:
    \begin{defn}
        \label{def:optimal_value}
        \label{def:optimal_q_value}
        The \textnormal{Optimal (Q-)Value} of a state(-action pair) is defined as:
        \begin{align}
            V^*(s;t) &= \max_{\pi} V^{\pi}(s;t) \\
            Q^*(s,a;t) &= \max_{\pi} Q^{\pi}(s,a;t).
        \end{align}
    \end{defn}

    In reinforcement learning, the objective is to find a policy with maximal value:
    \begin{defn}
        The \textnormal{(standard) reinforcement learning objective function} $J(\pi)$ is defined as:
        \begin{align}
            J(\pi) = V^{\pi}(s_0;0).
        \end{align}

        The objective of (standard) reinfrocement learning can then be stated as finding $\max_{\pi} J(\pi)$.
    \end{defn}

    It can be shown \todo{refs} that the optimal (Q-)value functions satisfy the \textit{Bellman equations}:
    \begin{align}
        V^*(s;t) &= \max_{a\in\cl{A}} Q^*(s,a;t), \\
        Q^*(s,a;t) &= R(s,a) + \bb{E}_{s'\sim \suc{s}{a}} [V^*(s';t+1)].
    \end{align} 

    In tabular reinforcement learning, a table of values for $V(s;t)$ \todo{havent actually defined V without any superscript} is kept for each $s,t$. Given any initial value function $V^{(0)}$ let the \textit{Bellman backup} operations be:
    \begin{align}
        V^{k+1}(s;t) &= \max_{a\in\cl{A}} Q^{k+1}(s,a;t), \\
        Q^{k+1}(s,a;t) &= \bb{E}_{s'\sim \suc{s}{a}} [R(s,a) + V^k(s';t+1)].
    \end{align}

    Using this \textit{dynamic programming} approach is known as \textit{value iteration}. It can be shown that the Bellman backups are contraction operators \todo{add cite}, which can be used to show that $V^{k}\rightarrow V^*$ as $k\rightarrow \infty$. In the discrete \todo{what the actual conditions are} case we are considering, there will always be some $N<\infty$ such that $V^{N}=V^*$.

    \todo{add optimal policy from Q values}

    \subsection{Maximum Entropy Reinforcement Learning}

        In \textit{Maximum Entropy Reinforcement Learning}, the objective function is altered to include the addition of an entropy term. Let $\cl{H}$ denote the (Shannon) entropy function \todo{cite}:
        \begin{align}
            \cl{H}(\pi(\cdot|s)) = \bb{E}_{a\sim\pi(\cdot|s)}[-\log \pi(a|s)].
        \end{align}

        Note that there are other forms of entropy, such as relative and Tsallis entropy, which can be used in place of Shannon entropy \todo{cite}. For the work considered in this thesis, the other forms of entropy can be used by replacing the definition of $\cl{H}$ by the relevant definition.

        In the maximum entropy objective, the relative weighting of entropy terms is included using a coefficient $\alpha$, which is called the \textit{temperature}. In the maximum entropy objective, analogues of the value functions can be defined, which are typically referred to as \textit{soft (Q-)values}, and similarly the maximum entropy objective is often referred to as the \textit{soft objective}.

        \begin{defn}
            \label{def:sft_value}
            \label{def:sft_q_value}
            The \textnormal{soft value} of a policy $\pi$ from state $s$ at time $t$ is:
            \begin{align}
                V_{\sft}^{\pi}(s;t) = \bb{E}_{\tau\sim\pi}\left[\sum_{i=t}^{H-1} r_t + \alpha\cl{H}(\pi(\cdot|s_i)) \Bigg| s_t=s \right].
            \end{align} 

            The \textnormal{soft Q-value} of a policy $\pi$, from state $s$, with action $a$, at time $t$ is:
            \begin{align}
                Q_{\sft}^{\pi}(s,a;t) = R(s,a) + \bb{E}_{s'\sim p(\cdot|s,a)} [V_{\sft}^{\pi}(s';t+1)].
            \end{align} 
        \end{defn}

        Similarly, optimal soft (Q-)values can be defined:
        \begin{defn}
            \label{def:optimal_sft_value}
            \label{def:optimal_sft_q_value}
            The \textnormal{Optimal soft (Q-)Value} of a state(-action pair) is defined as:
            \begin{align}
                V_{\sft}^*(s;t) &= \max_{\pi} V_{\sft}^{\pi}(s;t) \\
                Q_{\sft}^*(s,a;t) &= \max_{\pi} Q_{\sft}^{\pi}(s,a;t).
            \end{align}
        \end{defn}

        Equations similar to the Bellman equations, aptly named the \textit{Soft Bellman equations} can be defined, which differ to equations \todo{ref} by the replacement of the $\max$ operation with the \textit{softmax} operation (which is why the maximum entropy analogues are referred to as the \textit{soft} versions of their standard reinforcement learning counterparts).

        In maximum entropy reinforcement learning, the objective is to find a policy with maximal soft value:
        \begin{defn}
            The \textnormal{maximum entropy (or soft) reinforcement learning objective function} $J_{\sft}(\pi)$ is defined as:
            \begin{align}
                J_{\sft}(\pi) = V_{\sft}^{\pi}(s_0;0).
            \end{align}

            The objective of maximum entropy (or soft) reinfrocement learning can then be stated as finding $\max_{\pi} J_{\sft}(\pi)$.
        \end{defn}

        Similarly to standard reinforcement learning, it can be shown \todo{refs} that the optimal soft (Q-)value functions satisfy the \textit{soft Bellman equations}:
        \begin{align}
            V_{\sft}^*(s;t) &= \alpha \log \sum_{a\in\cl{A}} \exp\left( Q_{\sft}^*(s,a;t) / alpha \right), \\
            Q_{\sft}^*(s,a;t) &= R(s,a) + \bb{E}_{s'\sim \suc{s}{a}} [V_{\sft}^*(s';t+1)].
        \end{align} 

        Again, similarly to standard reinforcement learning, we can define \textit{soft Bellman backups} that admit an analogous algorithm to value iteration:
        \begin{align}
            V_{\sft}^{k+1}(s;t) &= \alpha \log \sum_{a\in\cl{A}} \exp\left( Q_{\sft}^{k+1}(s,a;t) / alpha \right), \\
            Q_{\sft}^{k+1}(s,a;t) &= R(s,a) + \bb{E}_{s'\sim \suc{s}{a}} [V_{\sft}^k(s';t+1)].
        \end{align}

        Finally, given the optimal soft value and soft Q-value functions, the optimal soft policy is known \todo{cite}:
        \begin{align}
            \pi_{\sft}^*(a|s;t) = \exp\left(\left(Q_{\sft}^*(s,a;t) - V_{\sft}^*(s;t)\right) / \alpha \right).
        \end{align}

    \subsection{Remaining todos for this chapter after first draft}

        \todo{Add a comment similar to DENTS paper where we will drop the t in notation. Make it quite bold somehow}


\section{Trial-Based Heuristic Tree Search and Monte-Carlo Tree Search}
\label{sec:2-2-thts}

    \todo{list}
    \begin{itemize}
        \item Give high level overview of MCTS (why use it etc)
        \item Outline that I'll present this as here is THTS, and then here's the THTS routines for MCTS
    \end{itemize}

    \todo{double check the intro to 2.2, as wrote this a while ago}

    \todo{try to make sure specific about using thts vs mcts}

    In this section we introduce \thtspp\ewe \cite{thtspp}, which is an open-source, parallelised extension of the  Trial-based Heuristic Tree Search schema \cite{thts} (THTS). This schema is a generalisation of Monte Carlo Tree Search (MCTS), as presented in Section \ref{sec:2-2-2-mcts}. In \thtspp\ewe trees consist of \textit{decision nodes} and \textit{chance nodes}. Decision nodes output actions that can be taken by the agent, and chance nodes output \textit{outcomes} that may be random and may depend on the action taken. As such, each decision node has an associated \textit{state} and each chance node has an associated \textit{state-action pair}. In this work, we are considering fully-observable environments, but \thtspp\ewe can be generalised to consider \textit{partially-observable} environments. We give \thtspp\ewe implementations of the standard Upper Confidence Bound applied to Trees (UCT) algorithm and Maximum ENtropy Tree Search (MENTS) in Sections \ref{sec:2-2-2-mcts} and \ref{sec:2-2-3-ments} respectively.

    In MCTS we run trials, either for some fixed number of trials, or some timelimit, where each trial is split into four stages: 
        (1) selection, which samples states and actions for the trial, corresponding to a path down the tree;
        (2) expansion, which creates any new nodes in the tree; 
        (3) initialisation, which initialises values at any new leaf nodes in the tree;
        (4) backup, which updates values at all nodes visited on the trial.

    \todo{add MCTS figure here?}

    \begin{lstlisting}
def foo(bar):
        print("helloworld!")
    \end{lstlisting}

    \subsection{Trial Based Heuristic Tree Search}
    \label{sec:2-2-1-thts}
    
        \todo{list}
        \begin{itemize}
            \item{Copy DENTS MCTS section presentation, make a notation } $\texttt{node}(s_t)$ for the node at state $s_t$
            \item Present thts++
            \item Indicate what parts are new versus the original paper (context function, optionally running \mctsmode\ewe and mutli-threading)
            \hide{\item Small comment about multi-threading and two-phase locking used to avoid deadlock}
            \hide{\item TODO: probably not necessary to say - but thought of nice/concise way of explaining it (a node can lock children, not parent, if need info from parent, then it has to put a thread safe copy in the context)}
            \item Define terms precisely and consistently, for example \mctsmode\ewe (say that notation and terminology varies widely in literature, e.g. does uct run in mcts mode or not?)
            \item Mention that $\Vinit$ can be implemented as $V_\theta$ to be used with deep RL methods
            \hide{\item \todo{Find the best place to talk about deep RL? Maybe in the RL section?}}
        \end{itemize}

        \todo{this section relly really needs diagrams}

        \todo{add psuedo code}

        In this section we will present \thtspp schema, which is \todo{adaptation?} of the Trial-Based Heuristic Tree Search (THTS) schema \todo{cite}. After we have presented \thtspp, we will use the schema to define tree search algorithms that are relevant in this thesis, namely Upper Confidence Bound Applied to Trees (UCT) \todo{cite} in subsection \todo{ref} and Maximum ENtropy Tree Search (MENTS) in subsection \todo{Ref}. Finally we will briefly point out the differences between \thtspp and the original THTS schema in subsection \todo{ref}.

        \todo{this is already a subsection, so update above}

        In \thtspp\ewe a search tree $\cl{T}$ is built using Monte Carlo trials. Each trial is split into two phases: the \textit{selection phase} where a trajectory is sampled using a \textit{search policy}; and the \textit{backup phase} where value estimates stored in the tree structure are updated. In \thtspp\ewe the selection phase encumpasses the expansion and initialisation phases \todo{of the common presentation of MCTS}, where new nodes are added to the tree and the values of any new leaf node is initialised. 

        \todo{be more presise about trajectory vs trial, and update for sect 2.1}.

        To simplify notation in the presentation of \thtspp\ewe we will assume that states and state-action pairs have a one to one correspondance with nodes in the search tree $\cl{T}$. This assumption is purely to simplify notation for a clean presentation, and any results discussed in this thesis generalise to when this assumption does not hold. Given this assumption, we can state that the search tree is a subset of the state and state-action spaces, that is $\cl{T}\subseteq \cl{S} \cup \cl{S} \times \cl{A}$. \todo{rephrase last sentance for defn}

        \begin{defn}
            A \textnormal{search tree} $\cl{T}$ is a subset of the state and state-action spaces, that is $\cl{T}\subseteq \cl{S} \cup \cl{S} \times \cl{A}$, where for each $s\in\cl{T}$, there exists some trajectory $\tau_{:h}$ such that $s_h = s$, each $s'\in\tau_{:h}$ is also in the tree $s'\in\cl{T}$ and each $s',a'\in\tau{:h}$ is also in the tree $(s',a')\in\cl{T}$.
        \end{defn}

        \todo{clean above defn up}

        \todo{probably want to explicitly define what it means for s or s,a to be in a trajectory}

        \todo{words about decision and chance nodeS?}

        \begin{defn}
            A \textnormal{decision node} refers to any state that is in the search tree: $s\in\cl{T}$. A \textnormal{chance node} refers to any state-action pair that is in the search tree: $(s,a)\in\cl{T}$. And a \textnormal{node} is used to refer to any decision or chance node in the tree. When it is not clear from context if an $s$ or $(s,a)$ refers to a state(-action pair), the notation $\node(s)$ and $\node(s,a)$ will be used. \todo{fix node notation here}
        \end{defn}

        Additionally, each decision and chance node will generally store value estimates that are algorithm dependent. To specify this we will use $\node(s).V$ to denote the set of values stored at node $\node(s)$, and $\node(s,a).Q$ for the set of value stored at node $\node(s,a)$. \todo{make this a defn?}

        \todo{define N(s) and N(s,a)}

        The initial search tree consists of a single root node that corresponds to the initial state of the MDP: $\cl{T}^0=\{s_0\}$. And let $\cl{T}^k$ denote the search tree of \thtspp\ewe after $k$ trials have been run.

        To specify an algorithm in the \thtspp\ewe schema, the following need to be provided:
        \begin{description}
            \item[Search policy:]
                A distribution  $\pi^{k}$ for the $(k+1)$th trial, which can use values in the current search tree $\cl{T}^{k}$;
            \item[Heuristic function:]
                A function $\Vinit$ used as a heuristic to initialise values for new desicion nodes added to the tree;
            \item[Backup function:]
                Two functions $\cl{B}_V$ and $\cl{B}_Q$ which updates values; \todo{Clewan this up, and word better pls}
            \item[MCTS mode:]
                A boolean \mctsmode specifying if \thtspp\ewe should operate in MCTS mode.
        \end{description}

        \todo{actually define the above things properly somewhere}

        The $k+1$th trial of the \thtspp\ewe schema operates as follows: \todo{this is probably better written as psuedocode...}
        \begin{enumerate}
            \item sample a trajectory $\tau_{:h}$ using the search policy $\pi^{k}$;
            \begin{itemize}
                \item If \mctsmode\ewe is False, then $h=H$;
                \item If \mctsmode\ewe is True, then $h$ is such that $s_{h-1}\in\cl{T}^k$ and $s_h\not\in\cl{T}^k$, or $h=H$.
            \end{itemize}
            \item Any new nodes nodes that need to be added from this trajectory are added to the tree, $\cl{T}^{k+1} = \cl{T}^k \cup \tau_{:h}$;
            \item If $s_h\not\in\cl{T}^k$ then $\node(s_h).V$ is initialised using $\Vinit$;
            \item The backup functions are used to update values in the tree:
            \begin{itemize}
                \item For $i={h-1,h-2,...,1,0}$:
                \begin{itemize}
                    \item $\node(s_i,a_i).Q \leftarrow \cl{B}_Q(\{\node(s').V | s'\in\node(s_i,a_i).\children\})$
                    \item $\node(s_i).V \leftarrow \cl{B}_V(\{\node(s_i,a').Q | a'\in\node(s_i).\children\})$
                \end{itemize}
            \end{itemize}
        \end{enumerate}

        \todo{converting the above into some psuedocode. Should probably define children as a property of nodes. Should also just state that } $\node(s_i,a_i).Q$ \todo{and so on are just scalar values for now. We can make them vectors when needed later}. \todo{also make sure define } $N(s)$ and $N(s,a)$
        \begin{lstlisting}
def run_trial(search_policy: $\pi$, heuristic_fn: $\Vinit$):
    $\tau_{:h}$ = sample_trajectory($pi$)
    if $s_h\not\in\cl{T}$:
        initialise_values($s_h$, $\Vinit$)
    for i in $\{$h-1,h-2,...,1,0$\}$:
        backup_q($s_i$,$a_i$)
        backup_v($s_i$)

def sample_trajectory(search_policy: $\pi$):
    pass

def initialise_values($s_h$, $\Vinit$):
    pass 

def backup_q($s_i$, $a_i$):
    pass
    
def backup_v($s_i$):
    pass
        \end{lstlisting}
        % TODO: make a figure env for code

        \todo{define } $\node(s_i).\children$ and $\node(s_i,a_i).\children$.

        \todo{Check how thts deals with using value estimates when children dont exist. There should be some form of using heuristic. Maybe this needs to be added to THTSpp todo list}


        \todo{make sure neurips paper writing integrated (commented out, and below this comment in the .tex) - read the actual pdf for thesis and neurips papers and compare cover same info?}
% MCTS methods build a search tree $\cl{T}$ using Monte-Carlo trials. Each trial is split into two phases: starting from the root node, actions are chosen according to a \textit{search policy} and states sampled from the transition distribution until the first state not in $\cl{T}$ is reached. A new node is added to $\cl{T}$ and its value is initialised using some function $V^{\text{init}}$, often using a \textit{rollout policy} to select actions until the time horizon $H$ is reached. In the second phase, the return for the trial is back-propagated up (or `backed up') the tree to update the values of nodes in $\cl{T}$. For a reader unfamiliar with MCTS, we refer to \cite{browne2012survey} for a review of the MCTS literature, as many variants of MCTS exist and may vary from our description. 

%         Two critical choices in designing an MCTS algorithm are the search policy (which needs to balance exploration and exploitation) and the backups (how values are updated). MCTS algorithms are often designed to achieve \textit{consistency} (i.e. convergence to the optimal action in the limit), which implies that running more trials will increase the probability that the optimal action is recommended.
%         % which gives the desirable property that running more trials should improve the policy.
        
%         To simplify notation we assume that each node in the search tree corresponds to a unique state, so we may represent nodes using states. Our algorithms and results do not make use of this assumption, and generalise to when this assumption does not hold.


    

    
    \subsection{Upper Confidence Bounds Applied to Trees (UCT)}
    \label{sec:2-2-2-uct}
    
        \todo{list}
        \begin{itemize}
            \item Define UCT here
        \end{itemize}

        \todo{I'm feeling ill writing this section, so just going to word vomit this shit out and make it sound not shit later}

        \todo{add Quct commands like Qments}

        Upper Confidence Bounds Applied to Trees (UCT) \todo{cite} is a commonly used tree search algorithm, which is based on the Upper Confidence Bounds (UCB) \todo{cite both papers} algorithm for Multi-Armed Bandit problems \todo{cite original MAB and a review}.

        In the literature, UCT and MCTS are often used synonomously, however this leaves some of the specifics of the algorithms used as ambiguous. In this thesis, we will present UCT as it was originally presented in \todo{cite}. And in subsection \todo{ref} we will specify the variant of UCT which is commonly referred to as MCTS.

        UCT can be defined using the THTS schema outlined in section \todo{ref} as follows:

        Firstly, UCT as originally presented is run with \mctsmode set to False. As such, all sampled trajectories are sampled until timestep $H$, the finite horizon of the MDP. 

        At each node a the sampled averages $\bar{V}_{\uct}$ or $\bar{Q}_{\uct}$ for value estimates.

        The search policy that UCT follows is:
        \begin{align}
            \pi_{\uct}(s) = \argmax_{a\in\cl{A}} Q_{\uct}(s,a) + b_{\uct} \sqrt{\frac{\log(N(s))}{N(s,a)}} 
        \end{align}
        \todo{add labels for equations}

        In \todo{ref above eqn}, when $N(s,a)=0$ there is a division by zero, which is taken as $\inf$, and ties are broken randomly, which effectively implements the ``every arm is initialised by pulling it once'' \todo{actually quote the paper, and cite UCT paper}. \todo{define the bias param}

        After sampling a trajectory $\tau_{:H}\sim\pi_{\uct}$ are updated as follows:
        \begin{align}
            \bar{Q}_{\uct}(s_t,a_t) &\leftarrow 
                \frac{1}{N(s,a)} \left( (N(s,a)-1) \bar{Q}_{\uct}(s_t,a_t) 
                    + \sum_{i=t}^{H-1} R(s_i,a_i) \right) \\
            \bar{V}_{\uct}(s_t) &\leftarrow 
                \frac{1}{N(s,a)} \left( (N(s,a)-1) \bar{V}_{\uct}(s_t,a_t) 
                    + \sum_{i=t}^{H-1} R(s_i,a_i) \right) 
        \end{align}  
        \todo{add labels for equations}

        \todo{Some note about the V values not actually being used in the algorithm}

        \todo{some comment about it can be implemented as backups (copy equations from THTS), but typically implemented as above. OR, just define backup functions to take the trajectory too}

        Because UCT is planning in a finite horizon MDP, the heuristic function will only be called on states that are at the time horizon H. As such, for UCT we can set $\Vinit(s) = 0$.

        \todo{add polynomial UCT here? and or prioritised UCT from alpha go here?}


        \todo{make sure neurips paper writing integrated in UCT and MCTS section (commented out, and below this comment in the .tex) - read the actual pdf for thesis and neurips papers and compare cover same info?}
%         \paragraph{UCT} 
%             UCT~\cite{kocsis2006uct} applies the upper confidence bound (UCB) in its search policy to balance exploration and exploitation. The $n$th trial of UCT operates as follows: let $\cl{T}$ be the current search tree and let $\tau=(s_0,a_0,...,a_{h-1},s_{h})$ denote the trajectory of the $n$th trial, where $s_h\not\in\cl{T}$ or $h=H$. At each node $s_t$ the UCT search policy $\pi_{\text{UCT}}$ will select a random action that has not previously been selected, otherwise, it will select the action with maximum UCB value:
%             %
% %            \begin{align}
% %                \pi_{\textnormal{UCT}}^n(s_t) &= \max_{a\in\cl{A}} \text{UCB}^n(s_t,a), \\
% %                \text{UCB}^n(s_t, a) &= \bar{Q}^{N(s_t,a)}(s_t, a)+c \sqrt{\frac{\log N(s_t)}{N(s_t, a)}},
% %            \end{align}
%             \begin{align}
%                 \pi_{\textnormal{UCT}}(s) &= \max_{a\in\cl{A}} \bar{Q}(s, a)+c \sqrt{\frac{\log N(s)}{N(s, a)}}, \label{eq:uct_distr}
%             \end{align}
%             %
%             \noindent where, $\bar{Q}(s,a)$ is the current empirical Q-value estimate, $N(s)$ (and $N(s,a)$) is how many times $s$ has been visited (and action~$a$ selected) and $c$ is an exploration parameter. Then, $s_h$ is added to the tree: $\cl{T}\leftarrow \{s_h\}\cup\cl{T}$. The backup consists of updating empirical estimates for $t=h-1,...,0$:
%             %
%             \begin{align}
%                 \bar{Q}(s_t, a_t) &\leftarrow \bar{Q}(s_t, a_t) + \frac{\bar{R}(t) - \bar{Q}(s_t, a_t)}{N(s_t, a_t) + 1}, \label{eq:uct_qbar}
%             \end{align}
%             %
%             \noindent where $\bar{R}(t) = V^{\text{init}}(s_h) + \sum_{i=t}^{h-1} R(s_i,a_i)$, and $V^{\text{init}}(s_h)=\sum_{i=h}^{H} R(s_i,a_i)$ if using a rollout policy.



    
    \subsection{Monte-Carlo Tree Search}
    \label{sec:2-2-3-mcts}

        \todo{list}
        \begin{itemize}
            \item Give overview of MCTS
            \item Give UCT in terms of THTS schema 
            \item Define terms precisely and consistently in terms of THTS functions, maybe \mctsmode\ewe should go here
            \item Define the value initialisation of THTS using a rollout policy for MCTS
            \item Talk about the things that are ambiguous from literature (e.g. people will just say UCT, which originally presented doesn't run in \mctsmode, but often assumed it does)
            \item Should talk about multi-armed bandits here?
        \end{itemize}

        \todo{I'm feeling ill writing this section, so just going to word vomit this shit out and make it sound not shit later}


        \todo{add Qmcts commands like Qments}

        In this thesis we will refer to any algorithm that only adds one decision node to the search tree on each trial as an MCTS algorithm. That is any THTS algorithm with \mctsmode set to True is an MCTS algorithm. 

        In this section we will present what is commonly referred to as MCTS in the literature, where the heuristic function is either in the form of a \textit{rollout}, using a \textit{rollout policy} \todo{cite papers that do this, including some that just call it UCT}, or use a function $V_\theta$ that aims to approximate the true optimal value function $V^*$ from Equation \todo{ref} \todo{cite papers that do this}.

        These algorithms follow the same 

        These algorithms use the similar value functions to UCT, $\bar{V}_{\mcts}$ or $\bar{Q}_{\mcts}$. 

        The search policy corresponds to the UCT search policy, using the new $\bar{Q}_{\mcts}$ values:
        \begin{align}
            \pi_{\mcts}(s) = \argmax_{a\in\cl{A}} Q_{\mcts}(s,a) + b_{\mcts} \sqrt{\frac{\log(N(s))}{N(s,a)}} 
        \end{align}
        \todo{add labels for equations}

        A trajectory $\tau_{:h}$ is sampled until a new decision node not in the tree is reached, as we are now running with \mctsmode set to True.

        If the algorithm uses a function approximation $V_\theta$, then it is used directly for the heuristic function $\Vinit$. If a rollout is used for the heuristic function, then the algorithm needs to define a \textit{rollout policy} $\pi_{\rollout}$, which is used to sample a Monte Carlo estimate of the value function $V^{\pi_{\rollout}}$ as follows. The sampled trajectory $\tau_{:h}\sim\pi_{\mcts}$ is extended with the rollout trajectory $\tau_{h:H}\sim\pi_{\rollout}$ to give the Monte Carlo estimate of the value at $s_h$:
        \begin{align}
            V^{\pi_{\rollout}}(s_h) \approx \sum_{i=h}^{H-1} r_i.
        \end{align}
        
        Letting $\tilde{r} = \Vinit(s_h)$, the value estimates (or sample averaqes) are updated as follows:

        \begin{align}
            \bar{Q}_{\mcts}(s_t,a_t) &\leftarrow 
                \frac{1}{N(s,a)} \left( (N(s,a)-1) \bar{Q}_{\mcts}(s_t,a_t) 
                    + \tilde{r} + \sum_{i=t}^{h-1} r_i \right) \\
            \bar{V}_{\mcts}(s_t) &\leftarrow 
                \frac{1}{N(s,a)} \left( (N(s,a)-1) \bar{V}_{\mcts}(s_t,a_t) 
                    + \tilde{r} + \sum_{i=t}^{h-1} r_i \right) 
        \end{align}  
        \todo{add labels for equations}

    
    \subsection{Maximum Entropy Tree Search}
    \label{sec:2-2-4-ments}
    
        \todo{list}
        \begin{itemize}
            \item Define MENTS here
        \end{itemize}

        \todo{I'm feeling ill writing this section, so just going to word vomit this shit out and make it sound not shit later}


        Maximum ENtropy Tree Search (MENTS) \todo{cite}, in contrast to UCT, focuses on the maximum-entropy objective. In its original presentation \mctsmode is set to True, and it uses the soft value estimates $\hat{V}_{\ments}$ and $\hat{Q}_{\ments}$. The MENTS search policy is
        \begin{align}
            \piments(a|s) &= 
                (1-\lambda_s)\exp\left(\frac{1}{\alpha_{\ments}}\left(\Qments(s,a)-\Vments(s)\right)\right) 
                    + \frac{\lambda_s}{|\cl{A}|},
        \end{align}
        \todo{add labels for equations}
        where $\alpha_{\ments}$ is the temperature paramter used for Equation \todo{ref} in MENTS, and $\lambda_s=\min(1,\epsilon/\log(e+N(s))),$ with $\epsilon \in (0,\infty)$ is an exploration parameter.

        The value estimates are updated using the soft Bellman backups (\todo{ref}) as follows:
        \begin{align}
            \Qments(s_t,a_t) &\leftarrow 
                R(s_t,a_t) + \sum_{s'\in\suc{s}{a}} \left( \frac{N(s')}{N(s_t,a_t)} \Vments(s') \right), \\
            \Vments(s_t) &\leftarrow 
                \alpha \log \sum_{a\in\cl{A}} \exp \left(\frac{1}{\alpha}\Qments(s_t,a) \right).
        \end{align}
        \todo{add labels for equations}

        \todo{talk about initialistations, for Vinit its the same as MCTS, think about how to integrate Qinit properly into the thesis using below stuff (commented out)}

            % Each $\Qst{s}{a}{}$ is initialised using another function $Q^{\text{init}}_{\text{sft}}(s,a)$ (but is typically zero).
%            Q-values can be initialised with an initial value given by $Q^{\text{init}}_{\text{sft}}(s,a)$, which is typically $0$, and $\Vst{s}{}$ is initi
%            Default soft Q-values (for when $N(s,a)=0$) can be set $\Qst{s}{a}{0}=Q^{\text{init}}_{\text{sft}}(s,a)$, but typically $Q^{\text{init}}_{\text{sft}}(s,a)=0$.
%            %, but if a policy network $\tilde{\pi}$ is available then $Q^{\text{init}}_{\text{sft}}(s,a)=\log \tilde{\pi}(a|s)$ can be used as suggested by Xiao \etal \cite{xiao2019maximum}. 
%            Initial values, $\Vst{s_{h+1}}{1}$, are set using an evaluation function or rollout policy.
%            % The initial value for the new node, $\Vst{s_{h+1}}{1}$, is set either using an evaluation function (e.g. a value network) or using the cumulative return from a rollout policy.% $\sum_{i=h+1}^H R(s_i,a_i)$. 



        \todo{make sure neurips paper writing integrated (commented out, and below this comment in the .tex) - read the actual pdf for thesis and neurips papers and compare cover same info?}
        %         \paragraph{MENTS} 
%             MENTS \cite{xiao2019maximum} combines maximum entropy policy optimization \cite{haarnoja2017reinforcement, ziebart2008maximum} with MCTS. Algorithmically, it is similar to UCT. The two differences are: (1) the search policy follows a stochastic Boltzmann policy, and (2) it uses soft values that are updated with dynamic programming backups. The MENTS search policy $\pi_{\textnormal{MENTS}}$ is given by:
%             %
%             \begin{align}
%                 \pi_{\textnormal{MENTS}}(a|s) &= (1-\lambda_s)\rho_{\textnormal{MENTS}}(a|s) + \frac{\lambda_s}{|\cl{A}|}, \\
%                 \rho_{\textnormal{MENTS}}(a|s) &= \exp\left(\frac{1}{\alpha}\left(\Qst{s}{a}{}-\Vst{s}{}\right)\right) \label{eq:rhosft}
%             \end{align}
%             %
%             \noindent where $\lambda_s=\min(1,\epsilon/\log(e+N(s))),$ $\epsilon \in (0,\infty)$ is an exploration parameter and $\Vst{s}{}$ (and $\Qst{s}{a}{}$) are the current soft (Q-)value estimates. 
%             %after $m_s$ visits to $s$ and $m_a$ visits to $(s,a)$, respectively. 
%             The soft value of the new node is initialised $\Vst{s_h}{}\leftarrow V^{\text{init}}(s_h)$ and the soft values are updated with backups for $t=h-1,...,0$:
%             %
%             \begin{align}
%                 \Qst{s_t}{a_t}{} &\leftarrow R(s_t,a_t) + \sum_{s'\in\succc{s}{a}} \left( \frac{N(s')}{N(s_t,a_t)} \Vst{s'}{} \right), \\ %\label{eq:soft_q_backup} \\
%                 \Vst{s_t}{} &\leftarrow \alpha \log \sum_{a\in\cl{A}} \exp \left(\frac{1}{\alpha}\Qst{s_t}{a}{} \right). \label{eq:soft_v_backup}
%             \end{align}
%             %
%             Each $\Qst{s}{a}{}$ is initialised using another function $Q^{\text{init}}_{\text{sft}}(s,a)$ (but is typically zero).
% %            Q-values can be initialised with an initial value given by $Q^{\text{init}}_{\text{sft}}(s,a)$, which is typically $0$, and $\Vst{s}{}$ is initi
% %            Default soft Q-values (for when $N(s,a)=0$) can be set $\Qst{s}{a}{0}=Q^{\text{init}}_{\text{sft}}(s,a)$, but typically $Q^{\text{init}}_{\text{sft}}(s,a)=0$.
% %            %, but if a policy network $\tilde{\pi}$ is available then $Q^{\text{init}}_{\text{sft}}(s,a)=\log \tilde{\pi}(a|s)$ can be used as suggested by Xiao \etal \cite{xiao2019maximum}. 
% %            Initial values, $\Vst{s_{h+1}}{1}$, are set using an evaluation function or rollout policy.
% %            % The initial value for the new node, $\Vst{s_{h+1}}{1}$, is set either using an evaluation function (e.g. a value network) or using the cumulative return from a rollout policy.% $\sum_{i=h+1}^H R(s_i,a_i)$. 



    
    



\clearpage
THINGS WROTE before for THTS section \todo{read through and see if anything want to keep, otherwise delete}
\clearpage







        
        In \thtspp\ewe we run trials for either some fixed number of trials $n$, or some time limit $T$. Each trial 
        consists of three steps: 
        (1) sample a context, which is used to store variables that are associated with a specific trial, and is passed to the following three functions;
        (2) selection, which samples states, actions and outcomes for the trial, corresponding to a path down the tree;
        (3) initialisation, which creates any new nodes in the tree and initialises their values;
        (4) backup, which updates values at all nodes visited on the trial.

        Decision nodes follow the interface:
        \begin{lstlisting}
class DNODE:
    # children : dictionary[$A$] -> DNODE
    def initialise(state $(s_t)$, depth $(t)$, context)
    def select_action(context)
    def backup(trial_return $(R_t)$, context)
        \end{lstlisting}
        % TODO: make a figure env for code

        And chance nodes:
        \begin{lstlisting}
class CNODE:
    # children : dictionary[$S$] -> DNODE
    def initialise(state $(s_t)$, action $(a_t)$, depth $(t)$, context)
    def sample_outcome(context)
    def backup(trial_return $(R_t)$, context)
        \end{lstlisting}
        % TODO: make a figure env for code

        The \runtrial function can be written as:
        \begin{lstlisting}
def run_trial:
    # root_node : DNODE
    # mcts_mode : bool
    t = 0
    state = root_node.state
    while (not selection_phase_ended(t,mcts_mode)):

def selection_phase_ended(t,mcts_mode):
    if 
        \end{lstlisting}


        urgh BRAIN POOP

        TODO - copy the descriptions from DENTS, and adapt and add the psuedocode








\section{Multi-Objective Reinforcement Learning}
\label{sec:2-3-morl}
    
    \todo{list}
    \begin{itemize}
        \item MOMDP definition
        \item (Expected) utility
        \item Define an interface for pareto front and convex hull objects
        \item Define CHVI
        \item Should talk about multi-objective and/or contextual multi-armed bandits here?
        \item I'm planning on aligning this section with the recent MORL survey \cite{morl_survey}
        \item Mention some deep MORL stuff, say that this work (given AlphaZero) is adjacent work
    \end{itemize}

    \todo{Follow CHMCTS and https://arxiv.org/abs/2103.09568}

    In this thesis we will follow a utility based approach to Multi-Objective Reinforcement learning similar to \todo{cite}. For a full review of Multi-Objective Reinforcement Learning see \todo{cite}. In this work we will specifically consider \textit{linear utility} functions and the decision support scenario, which will be defined more precisely below.
    
    \todo{MOMDP}

    \todo{utility}

    \todo{linear utility}

    \todo{solution sets and convex hulls}

    \todo{decision support scenario}

    \todo{subsection CHVI}




\section{Multi-Objective Monte Carlo Tree Search}
\label{sec:2-4-momcts}

    \todo{I think this whole section can just go in litrev}

    \todo{list}
    \begin{itemize}
        \item Define the old methods (using the CH object methods, so clear that not doing direct arithmetic)
        \item Mention that old method could be written using the arithmetic of CHMCTS (but they don't) 
        \hide{\item TODO: write about \& make sure its implemented - its because just updating for 1 is more efficient in deterministic, and say that the additions can be implemented as updating for 1 value when determinstic}
        \item Different flavours copy UCT action selection, but with different variants
        \item Link back to contributions and front load our results showing that all of the old methods don't explore correctly
    \end{itemize}

    \todo{There has been some prior work in multi-objective MCTS which we will outline here}

    \todo{Write out implementations of prior works using THTS}





\section{Sampling Random Variables}
\label{sec:2-5-sampling}

    \todo{list}
    \begin{itemize}
        \item Talk about the alias method here
        \item Reference to chapter \ref{ch:4-dents} section where talk about using this with THTS
    \end{itemize}