% \begin{savequote}[8cm]
% Alles Gescheite ist schon gedacht worden.\\
% Man muss nur versuchen, es noch einmal zu denken.

% All intelligent thoughts have already been thought;\\
% what is necessary is only to try to think them again.
%   \qauthor{--- Johann Wolfgang von Goethe \cite{von_goethe_wilhelm_1829}}
% \end{savequote}

\chapter{\label{ch:2-background}Background}

    \minitoc

    \todo{Introduce that going to introduce notation and give the building blocks this thesis builds off}

    \todo{Nicks comment to revisit: In Sections 2 and 3 you present RL before tree search. This was a surprise to me. Wouldn’t it be better to start with single objective MDPs, then introduce methods of solving them (planning, bandits, rl) and the different assumptions they make? Then move to MO versions. I’d also be open to you presenting MCTS then generalising to MCTS rather than the other way around, but I think the way you present it is probably the most efficient.}

\section{Multi-Armed Bandits}
\label{sec:2-0-mab}

    \todo{Introduce tree search using multi-armed bandits?}
    \hide{
    \begin{itemize}
        \item Would like to think a bit about some of the bandits work that sample actions (from adversarial I think), because they were similar to boltzmann search but I hadn't seen details about those works when writing dents
        \item Also the gradient based MAB stuff in sutton and barto book? Looks relevant? Maybe consider that as update to DENTS paper? Either way, another idea for getting good Go results.
    \end{itemize}
    }
    \todo{list}
    \begin{itemize}
        \item $R(s,a)$ is a random variable in MAB literature, but we're assuming it's a fixed value in RL
        \item Multi-Armed Bandits routines algos
        \item Exploring Bandits routines and algos
        \item Contextual Bandits routines and algos
    \end{itemize}

\section{Reinforcement Learning}
\label{sec:2-1-rl}

    \todo{list}
    \begin{itemize}
        \item Typical agent interacting with environment diagram 
        \item Agent planning with simulator 
        \item MDPs definition
        \item Value functions (single and multi-objective)
        \item Basic results and definitions we use (tabular planning algorithms)
        \item Talk about entropy and some of that work (probably a subsection)
    \end{itemize}

    \begin{defn}
        \label{def:mdp}
        A \textnormal{Markov Decision Process} is a tuple $\cl{M}=(\cl{S},s_0,\cl{A},\cl{R},p,H)$, where $\cl{S}$ is a 
        set of states, $s_0\in\cl{S}$ is an initial state, $\cl{A}$ is a set of actions, $\cl{R}(s,a)$ is a reward 
        function $\cl{S}\times \cl{A}\rightarrow \bb{R}$, $p(\cdot | s,a)$ is a next state transition distribution 
        $\cl{S} \times \cl{A} \times \cl{S} \rightarrow [0,1]$ and $H\in\bb{N}$ is a finite-horizon time bound. 
    \end{defn}

    \todo{Define standard things} 
    1. value functions

    \todo{Split this into MDPs definition and move RL below monte carlo tree search?}


\section{Trial-Based Heuristic Tree Search and Monte-Carlo Tree Search}
\label{sec:2-2-thts}

    \todo{list}
    \begin{itemize}
        \item Give high level overview of MCTS (why use it etc)
        \item Outline that I'll present this as here is THTS, and then here's the THTS routines for MCTS
    \end{itemize}

    In this section we introduce \thtspp\ewe \cite{thtspp}, which is an open-source, parallelised extension of the  Trial-based Heuristic Tree Search schema \cite{thts} (THTS). This schema is a generalisation of Monte Carlo Tree Search (MCTS), as presented in Section \ref{sec:2-2-2-mcts}. In \thtspp\ewe trees consist of \textit{decision nodes} and \textit{chance nodes}. Decision nodes output actions that can be taken by the agent, and chance nodes output \textit{outcomes} that may be random and may depend on the action taken. As such, each decision node has an associated \textit{state} and each chance node has an associated \textit{state-action pair}. In this work, we are considering fully-observable environments, but \thtspp\ewe can be generalised to consider \textit{partially-observable} environments. We give \thtspp\ewe implementations of the standard Upper Confidence Bound applied to Trees (UCT) algorithm and Maximum ENtropy Tree Search (MENTS) in Sections \ref{sec:2-2-2-mcts} and \ref{sec:2-2-3-ments} respectively.

    In MCTS we run trials, either for some fixed number of trials, or some timelimit, where each trial is split into four stages: 
        (1) selection, which samples states and actions for the trial, corresponding to a path down the tree;
        (2) expansion, which creates any new nodes in the tree; 
        (3) initialisation, which initialises values at any new leaf nodes in the tree;
        (4) backup, which updates values at all nodes visited on the trial.

    \todo{add MCTS figure here?}

    \begin{lstlisting}
def foo(bar):
        print("helloworld!")
    \end{lstlisting}

    \subsection{Trial Based Heuristic Tree Search}
    \label{sec:2-2-1-thts}
    
        \todo{list}
        \begin{itemize}
            \item{Copy DENTS MCTS section presentation, make a notation } $\texttt{node}(s_t)$ for the node at state $s_t$
            \item Present thts++
            \item Indicate what parts are new versus the original paper (context function, optionally running \mctsmode\ewe and mutli-threading)
            \hide{\item Small comment about multi-threading and two-phase locking used to avoid deadlock}
            \hide{\item TODO: probably not necessary to say - but thought of nice/concise way of explaining it (a node can lock children, not parent, if need info from parent, then it has to put a thread safe copy in the context)}
            \item Define terms precisely and consistently, for example \mctsmode\ewe (say that notation and terminology varies widely in literature, e.g. does uct run in mcts mode or not?)
            \item Mention that $\Vinit$ can be implemented as $V_\theta$ to be used with deep RL methods
            \hide{\item \todo{Find the best place to talk about deep RL? Maybe in the RL section?}}
        \end{itemize}
        
        \todo{To simplify notation we're going to assume that states and state-action pairs have a one to one correspondance with decision and chance nodes respectively. This is just to make notation clean, results generalse. AND} were going to use \node\ewe as a mapping from $\cl{S}\cup \cl{S}\times\cl{A}$, to \thtspp\ewe decision and chance nodes. So $\node(s_i)$ refers to the decision node associated with $\node(s_i)$ and $\node(s_i,a_{i+1})$ is a chance node.  \todo{Basically want to say that I'll use it without the node bit, unless I want to distinguish between the node, the state and state-action pair}.

        \todo{Define some node stuff here, } mabye define $\node(s_i).V$ and so on

        \todo{Add parameters of nodes in tables, and define the interfaces somewhere.}

        \todo{The initial tree is } $\cl{T}_0=\{\node(s_0)\}$.

        \todo{Define } the value which could be a convex hull and so on. \todo{Going to have to make below consistent with this}

        An algorithm following the \thtspp\ewe schema needs to define the following functions \todo{clean up this list}:
        \begin{description}
            \item[Search policy:]
                A distribution  $\pi^{k}$ for the $k$th trial, which can use values in the current search tree $\cl{T}_{k-1}$;
            \item[Initialisation function:]
                A function $\Vinit$ to initialise values for new desicion nodes added to the tree;
            \item[Backup function:]
                Two functions $\cl{B}^V$ and $\cl{B}^Q$ which updates values. \todo{Clewan this up, maybe just accept overloeaded notation?}

        \end{description}

        In \thtspp\ewe a search tree $\cl{T}$ is built using random trials. The $k$th trial is split into four phases:
        
        \begin{description}
            \item[Selection:] 
                A \textit{search policy} is used to sample actions and the transition distribution $p$ is used to sample successor states \todo{until new node not in tree, or some depth D}. From this we get a trajectory for the trial $\tau=(s_0,a_1,s_1,...,s_{h-1},a_h,s_h)$;
            \item[Expansion:]
                New nodes corresponding to the sampled trial trajectory are added to the tree: $\cl{T}_k = \cl{T}_{k-1} \cup \node(\tau)$, where $\node(\tau)$ is the set of nodes sampled in the trial (i.e. $\node(\tau) = \{\node(s_i)\}_{i=0}^h \cup \{\node(s_i,a_{i+1})\}_{i=0}^{h-1}$);
            \item[Initialisation:]
                If $s_h$ is a new node added to the tree ($\node(s_h)\not\in\cl{T}_{k-1}$), then initialise the value of $\node(s_h)$ using $\Vinit$;
            \item[Backup:]
                \todo{clean up value function notation}. $V^{k}(s_t) = \cl{B}^V(\mathbf{Q}^{k}(s_t,\cdot))$ and $Q^{k}(s_t) = \cl{B}^Q(\mathbf{V}^{k}(\cdot))$.
        \end{description}


        % MCTS methods build a search tree $\cl{T}$ using Monte-Carlo trials. Each trial is split into two phases: starting from the root node, actions are chosen according to a \textit{search policy} and states sampled from the transition distribution until the first state not in $\cl{T}$ is reached. A new node is added to $\cl{T}$ and its value is initialised using some function $V^{\text{init}}$, often using a \textit{rollout policy} to select actions until the time horizon $H$ is reached. In the second phase, the return for the trial is back-propagated up (or `backed up') the tree to update the values of nodes in $\cl{T}$. For a reader unfamiliar with MCTS, we refer to \cite{browne2012survey} for a review of the MCTS literature, as many variants of MCTS exist and may vary from our description. 

%         Two critical choices in designing an MCTS algorithm are the search policy (which needs to balance exploration and exploitation) and the backups (how values are updated). MCTS algorithms are often designed to achieve \textit{consistency} (i.e. convergence to the optimal action in the limit), which implies that running more trials will increase the probability that the optimal action is recommended.
%         % which gives the desirable property that running more trials should improve the policy.
        
%         To simplify notation we assume that each node in the search tree corresponds to a unique state, so we may represent nodes using states. Our algorithms and results do not make use of this assumption, and generalise to when this assumption does not hold.
    
%         \paragraph{UCT} 
%             UCT~\cite{kocsis2006uct} applies the upper confidence bound (UCB) in its search policy to balance exploration and exploitation. The $n$th trial of UCT operates as follows: let $\cl{T}$ be the current search tree and let $\tau=(s_0,a_0,...,a_{h-1},s_{h})$ denote the trajectory of the $n$th trial, where $s_h\not\in\cl{T}$ or $h=H$. At each node $s_t$ the UCT search policy $\pi_{\text{UCT}}$ will select a random action that has not previously been selected, otherwise, it will select the action with maximum UCB value:
%             %
% %            \begin{align}
% %                \pi_{\textnormal{UCT}}^n(s_t) &= \max_{a\in\cl{A}} \text{UCB}^n(s_t,a), \\
% %                \text{UCB}^n(s_t, a) &= \bar{Q}^{N(s_t,a)}(s_t, a)+c \sqrt{\frac{\log N(s_t)}{N(s_t, a)}},
% %            \end{align}
%             \begin{align}
%                 \pi_{\textnormal{UCT}}(s) &= \max_{a\in\cl{A}} \bar{Q}(s, a)+c \sqrt{\frac{\log N(s)}{N(s, a)}}, \label{eq:uct_distr}
%             \end{align}
%             %
%             \noindent where, $\bar{Q}(s,a)$ is the current empirical Q-value estimate, $N(s)$ (and $N(s,a)$) is how many times $s$ has been visited (and action~$a$ selected) and $c$ is an exploration parameter. Then, $s_h$ is added to the tree: $\cl{T}\leftarrow \{s_h\}\cup\cl{T}$. The backup consists of updating empirical estimates for $t=h-1,...,0$:
%             %
%             \begin{align}
%                 \bar{Q}(s_t, a_t) &\leftarrow \bar{Q}(s_t, a_t) + \frac{\bar{R}(t) - \bar{Q}(s_t, a_t)}{N(s_t, a_t) + 1}, \label{eq:uct_qbar}
%             \end{align}
%             %
%             \noindent where $\bar{R}(t) = V^{\text{init}}(s_h) + \sum_{i=t}^{h-1} R(s_i,a_i)$, and $V^{\text{init}}(s_h)=\sum_{i=h}^{H} R(s_i,a_i)$ if using a rollout policy.
    
    
%         \paragraph{MENTS} 
%             MENTS \cite{xiao2019maximum} combines maximum entropy policy optimization \cite{haarnoja2017reinforcement, ziebart2008maximum} with MCTS. Algorithmically, it is similar to UCT. The two differences are: (1) the search policy follows a stochastic Boltzmann policy, and (2) it uses soft values that are updated with dynamic programming backups. The MENTS search policy $\pi_{\textnormal{MENTS}}$ is given by:
%             %
%             \begin{align}
%                 \pi_{\textnormal{MENTS}}(a|s) &= (1-\lambda_s)\rho_{\textnormal{MENTS}}(a|s) + \frac{\lambda_s}{|\cl{A}|}, \\
%                 \rho_{\textnormal{MENTS}}(a|s) &= \exp\left(\frac{1}{\alpha}\left(\Qst{s}{a}{}-\Vst{s}{}\right)\right) \label{eq:rhosft}
%             \end{align}
%             %
%             \noindent where $\lambda_s=\min(1,\epsilon/\log(e+N(s))),$ $\epsilon \in (0,\infty)$ is an exploration parameter and $\Vst{s}{}$ (and $\Qst{s}{a}{}$) are the current soft (Q-)value estimates. 
%             %after $m_s$ visits to $s$ and $m_a$ visits to $(s,a)$, respectively. 
%             The soft value of the new node is initialised $\Vst{s_h}{}\leftarrow V^{\text{init}}(s_h)$ and the soft values are updated with backups for $t=h-1,...,0$:
%             %
%             \begin{align}
%                 \Qst{s_t}{a_t}{} &\leftarrow R(s_t,a_t) + \sum_{s'\in\succc{s}{a}} \left( \frac{N(s')}{N(s_t,a_t)} \Vst{s'}{} \right), \\ %\label{eq:soft_q_backup} \\
%                 \Vst{s_t}{} &\leftarrow \alpha \log \sum_{a\in\cl{A}} \exp \left(\frac{1}{\alpha}\Qst{s_t}{a}{} \right). \label{eq:soft_v_backup}
%             \end{align}
%             %
%             Each $\Qst{s}{a}{}$ is initialised using another function $Q^{\text{init}}_{\text{sft}}(s,a)$ (but is typically zero).
% %            Q-values can be initialised with an initial value given by $Q^{\text{init}}_{\text{sft}}(s,a)$, which is typically $0$, and $\Vst{s}{}$ is initi
% %            Default soft Q-values (for when $N(s,a)=0$) can be set $\Qst{s}{a}{0}=Q^{\text{init}}_{\text{sft}}(s,a)$, but typically $Q^{\text{init}}_{\text{sft}}(s,a)=0$.
% %            %, but if a policy network $\tilde{\pi}$ is available then $Q^{\text{init}}_{\text{sft}}(s,a)=\log \tilde{\pi}(a|s)$ can be used as suggested by Xiao \etal \cite{xiao2019maximum}. 
% %            Initial values, $\Vst{s_{h+1}}{1}$, are set using an evaluation function or rollout policy.
% %            % The initial value for the new node, $\Vst{s_{h+1}}{1}$, is set either using an evaluation function (e.g. a value network) or using the cumulative return from a rollout policy.% $\sum_{i=h+1}^H R(s_i,a_i)$. 
    



\clearpage
THINGS WROTE before
\clearpage







        
        In \thtspp\ewe we run trials for either some fixed number of trials $n$, or some time limit $T$. Each trial 
        consists of three steps: 
        (1) sample a context, which is used to store variables that are associated with a specific trial, and is passed to the following three functions;
        (2) selection, which samples states, actions and outcomes for the trial, corresponding to a path down the tree;
        (3) initialisation, which creates any new nodes in the tree and initialises their values;
        (4) backup, which updates values at all nodes visited on the trial.

        Decision nodes follow the interface:
        \begin{lstlisting}
class DNODE:
    # children : dictionary[$A$] -> DNODE
    def initialise(state $(s_t)$, depth $(t)$, context)
    def select_action(context)
    def backup(trial_return $(R_t)$, context)
        \end{lstlisting}
        % TODO: make a figure env for code

        And chance nodes:
        \begin{lstlisting}
class CNODE:
    # children : dictionary[$S$] -> DNODE
    def initialise(state $(s_t)$, action $(a_t)$, depth $(t)$, context)
    def sample_outcome(context)
    def backup(trial_return $(R_t)$, context)
        \end{lstlisting}
        % TODO: make a figure env for code

        The \runtrial function can be written as:
        \begin{lstlisting}
def run_trial:
    # root_node : DNODE
    # mcts_mode : bool
    t = 0
    state = root_node.state
    while (not selection_phase_ended(t,mcts_mode)):

def selection_phase_ended(t,mcts_mode):
    if 
        \end{lstlisting}


        urgh BRAIN POOP

        TODO - copy the descriptions from DENTS, and adapt and add the psuedocode



    \subsection{Monte-Carlo Tree Search}
    \label{sec:2-2-2-mcts}

        \todo{list}
        \begin{itemize}
            \item Give overview of MCTS
            \item Give UCT in terms of THTS schema 
            \item Define terms precisely and consistently in terms of THTS functions, maybe \mctsmode\ewe should go here
            \item Define the value initialisation of THTS using a rollout policy for MCTS
            \item Talk about the things that are ambiguous from literature (e.g. people will just say UCT, which originally presented doesn't run in \mctsmode, but often assumed it does)
            \item Should talk about multi-armed bandits here?
        \end{itemize}
    
    \subsection{Maximum Entropy Tree Search}
    \label{sec:2-2-3-ments}
    
        \todo{list}
        \begin{itemize}
            \item Define MENTS here
        \end{itemize}

\section{Multi-Objective Reinforcement Learning}
\label{sec:2-3-morl}
    
    \todo{list}
    \begin{itemize}
        \item MOMDP definition
        \item (Expected) utility
        \item Define an interface for pareto front and convex hull objects
        \item Define CHVI
        \item Should talk about multi-objective and/or contextual multi-armed bandits here?
        \item I'm planning on aligning this section with the recent MORL survey \cite{morl_survey}
        \item Mention some deep MORL stuff, say that this work (given AlphaZero) is adjacent work
    \end{itemize}

\section{Multi-Objective Monte Carlo Tree Search}
\label{sec:2-4-momcts}

    \todo{I think this whole section can just go in litrev}

    \todo{list}
    \begin{itemize}
        \item Define the old methods (using the CH object methods, so clear that not doing direct arithmetic)
        \item Mention that old method could be written using the arithmetic of CHMCTS (but they don't) 
        \hide{\item TODO: write about \& make sure its implemented - its because just updating for 1 is more efficient in deterministic, and say that the additions can be implemented as updating for 1 value when determinstic}
        \item Different flavours copy UCT action selection, but with different variants
        \item Link back to contributions and front load our results showing that all of the old methods don't explore correctly
    \end{itemize}

\section{Sampling Random Variables}
\label{sec:2-5-sampling}

    \todo{list}
    \begin{itemize}
        \item Talk about the alias method here
        \item Reference to chapter \ref{ch:4-dents} section where talk about using this with THTS
    \end{itemize}